import asyncio
import json
import os
from pathlib import Path
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from datetime import datetime

def load_urls_from_level4_json(json_file):
    """
    Load URLs from Level 4 extraction JSON file (from source field in Q&A pairs)
    """
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        urls = []
        
        # Extract URLs from source field in Q&A pairs
        if isinstance(data, dict) and 'qa_pairs' in data:
            for qa in data['qa_pairs']:
                if isinstance(qa, dict) and 'source' in qa:
                    source_url = qa['source']
                    if source_url and source_url.startswith('http'):
                        urls.append({
                            'url': source_url,
                            'text': qa.get('question', 'No description')[:100],
                            'category': qa.get('category', 'unknown'),
                            'priority': qa.get('priority', 3),
                            'from_file': Path(json_file).name
                        })
        
        return urls
    except Exception as e:
        print(f"âš ï¸ Error loading URLs from {json_file}: {e}")
        return []

async def crawl_url(crawler, url_info, output_dir):
    """
    Crawl a single URL and save the result
    """
    url = url_info['url']
    text = url_info['text']
    category = url_info['category']
    from_file = url_info['from_file']
    
    try:
        print(f"ğŸ” Crawling: {text}")
        print(f"   URL: {url}")
        print(f"   From: {from_file}")
        
        # Configure crawler run
        run_config = CrawlerRunConfig()
        
        # Run the crawler
        result = await crawler.arun(url=url, config=run_config)
        
        if result.success:
            # Create filename from URL
            url_filename = url.replace("://", "_").replace("/", "_").replace("?", "_").replace("=", "_")
            if len(url_filename) > 150:
                url_filename = url_filename[:150]
            
            # Save markdown content
            md_file = output_dir / f"{url_filename}.md"
            with open(md_file, "w", encoding="utf-8") as f:
                f.write(f"# {text}\n\n")
                f.write(f"**URL:** {url}\n")
                f.write(f"**Category:** {category}\n")
                f.write(f"**Source File:** {from_file}\n")
                f.write(f"**Crawled:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("---\n\n")
                f.write(result.markdown)
            
            print(f"âœ… Success: {len(result.markdown)} characters")
            print(f"   Saved to: {md_file.name}")
            
            return {
                'url': url,
                'text': text,
                'category': category,
                'from_file': from_file,
                'success': True,
                'content_length': len(result.markdown),
                'file': str(md_file)
            }
        else:
            print(f"âŒ Failed: {result.error_message}")
            return {
                'url': url,
                'text': text,
                'category': category,
                'from_file': from_file,
                'success': False,
                'error': result.error_message
            }
            
    except Exception as e:
        print(f"âŒ Error crawling {url}: {e}")
        return {
            'url': url,
            'text': text,
            'category': category,
            'from_file': from_file,
            'success': False,
            'error': str(e)
        }

async def main():
    """
    Main function to crawl URLs from Level 4 extracted JSON files and save to Level 5
    """
    print("ğŸš€ Starting Level 5 crawling from Level 4 extracted files...")
    print("ğŸ“ Results will be saved to Level 5 folder")
    
    # Find all JSON files in extracted_level4 folder
    level4_dir = Path("output/extracted_level4")
    if not level4_dir.exists():
        print(f"âŒ Directory {level4_dir} not found!")
        return
    
    json_files = list(level4_dir.glob("*.json"))
    # Exclude the combined file for individual processing
    individual_files = [f for f in json_files if "combined" not in f.name]
    
    print(f"ğŸ“ Found {len(individual_files)} individual JSON files in extracted_level4:")
    for file in individual_files[:5]:  # Show first 5
        print(f"   âœ… {file.name}")
    if len(individual_files) > 5:
        print(f"   ... and {len(individual_files) - 5} more files")
    
    # Also check the combined file
    combined_file = level4_dir / "level4_combined_extracted.json"
    if combined_file.exists():
        print(f"ğŸ“„ Using combined file: {combined_file.name}")
        json_files = [combined_file]
    else:
        json_files = individual_files
    
    # Collect all URLs from source fields
    all_urls = []
    for json_file in json_files:
        urls = load_urls_from_level4_json(json_file)
        print(f"ğŸ“ {json_file.name}: {len(urls)} URLs from sources")
        all_urls.extend(urls)
    
    # Remove duplicates based on URL
    unique_urls = []
    seen_urls = set()
    for url_info in all_urls:
        url = url_info['url']
        if url not in seen_urls:
            seen_urls.add(url)
            unique_urls.append(url_info)
    
    print(f"\nğŸ“Š URL Statistics:")
    print(f"   ğŸ”— Total URLs found: {len(all_urls)}")
    print(f"   ğŸ”„ Unique URLs: {len(unique_urls)}")
    print(f"   âŒ Duplicates removed: {len(all_urls) - len(unique_urls)}")
    
    if not unique_urls:
        print("âŒ No URLs found to crawl!")
        return
    
    # Sort by priority (1 = highest priority)
    unique_urls.sort(key=lambda x: x['priority'])
    
    # Show URLs to be crawled
    print(f"\nğŸ”— URLs to crawl (sorted by priority):")
    for i, url_info in enumerate(unique_urls[:10], 1):
        print(f"   {i}. Priority {url_info['priority']}: {url_info['url']}")
        print(f"      Question: {url_info['text'][:60]}...")
    if len(unique_urls) > 10:
        print(f"   ... and {len(unique_urls) - 10} more URLs")
    
    # Create Level 5 output directory
    crawl_output_dir = Path("output/crawled_pages_level5")
    crawl_output_dir.mkdir(exist_ok=True, parents=True)
    
    # Configure browser
    browser_config = BrowserConfig(
        headless=True,
        verbose=False
    )
    
    # Start crawling
    print(f"\nğŸ” Starting to crawl {len(unique_urls)} unique URLs...")
    print(f"ğŸ“ Results will be saved in: {crawl_output_dir}")
    
    results = []
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        for i, url_info in enumerate(unique_urls, 1):
            print(f"\nğŸ“„ [{i}/{len(unique_urls)}] Priority {url_info['priority']}")
            
            result = await crawl_url(crawler, url_info, crawl_output_dir)
            results.append(result)
            
            # Small delay to be respectful
            await asyncio.sleep(1)
    
    # Save crawling summary
    summary = {
        "crawl_date": datetime.now().isoformat(),
        "source": "Level 4 extracted JSON files",
        "level": "Level 5",
        "total_urls": len(unique_urls),
        "successful": len([r for r in results if r['success']]),
        "failed": len([r for r in results if not r['success']]),
        "results": results
    }
    
    summary_file = crawl_output_dir / "level5_crawl_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    # Show final statistics
    successful = [r for r in results if r['success']]
    failed = [r for r in results if not r['success']]
    
    print(f"\nğŸ‰ Level 5 Crawling completed!")
    print(f"ğŸ“Š Final Statistics:")
    print(f"   âœ… Successful: {len(successful)}")
    print(f"   âŒ Failed: {len(failed)}")
    print(f"   ğŸ“ Files saved in: {crawl_output_dir}")
    print(f"   ğŸ“„ Summary: {summary_file}")
    
    if successful:
        total_content = sum(r['content_length'] for r in successful)
        print(f"   ğŸ“ Total content: {total_content:,} characters")
        
        print(f"\nğŸ“ Sample successful crawls:")
        for result in successful[:5]:
            print(f"   âœ… {result['text'][:50]}... ({result['content_length']:,} chars)")
    
    if failed:
        print(f"\nâŒ Failed URLs:")
        for result in failed[:3]:
            print(f"   âŒ {result['text'][:50]}... - {result.get('error', 'Unknown error')}")
    
    print(f"\nğŸ’¡ Next step: Extract Q&A from Level 5 markdown files!")
    print(f"ğŸ’¡ You can use a similar extraction script for Level 5")

if __name__ == "__main__":
    asyncio.run(main()) 
import asyncio
import json
import os
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Set
import re
from difflib import SequenceMatcher

import openai
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from tenacity import retry, stop_after_attempt, wait_exponential


class IntentQuestion(BaseModel):
    """Model for a single intent question"""
    text: str
    entities: List[str] = Field(default_factory=list)
    is_template: bool = False
    source: str = "generated"
    confidence: float = 0.9


class IntentCategory(BaseModel):
    """Model for an intent category"""
    intent_id: str
    intent_name: str
    description: str
    entities_required: List[str]
    keywords: List[str]
    questions: List[IntentQuestion]
    examples_needed: int = 50  # Target number of questions per intent


class IntentDataset(BaseModel):
    """Model for the complete intent dataset"""
    metadata: Dict[str, Any]
    intent_categories: List[IntentCategory]
    total_questions: int = 0


# Define style patterns for question variations
style_patterns = {
    "formal": [
        "Xin cho bi·∫øt", "Vui l√≤ng cho bi·∫øt", "K√≠nh mong qu√Ω th·∫ßy/c√¥ cho bi·∫øt",
        "T√¥i mu·ªën h·ªèi", "T√¥i xin ph√©p ƒë∆∞·ª£c h·ªèi", "T√¥i th·∫Øc m·∫Øc",
        "Xin ƒë∆∞·ª£c h·ªèi", "Cho ph√©p t√¥i h·ªèi", "T√¥i c√≥ th·∫Øc m·∫Øc",
        "Mong ƒë∆∞·ª£c gi·∫£i ƒë√°p", "Xin ƒë∆∞·ª£c t∆∞ v·∫•n", "T√¥i mu·ªën t√¨m hi·ªÉu",
        "Cho t√¥i h·ªèi th√¥ng tin v·ªÅ", "Xin ƒë∆∞·ª£c t∆∞ v·∫•n v·ªÅ", "Mong ƒë∆∞·ª£c h∆∞·ªõng d·∫´n v·ªÅ",
        "T√¥i ƒëang quan t√¢m ƒë·∫øn", "T√¥i mu·ªën ƒë∆∞·ª£c t∆∞ v·∫•n v·ªÅ", "Xin h·ªèi th√¥ng tin chi ti·∫øt v·ªÅ"
    ],
    "informal": [
        "Cho m√¨nh h·ªèi", "Cho em h·ªèi", "Cho t√¥i h·ªèi",
        "M√¨nh mu·ªën bi·∫øt", "Em mu·ªën h·ªèi", "T√¥i mu·ªën h·ªèi",
        "Ch·ªã ∆°i cho em h·ªèi", "Anh ∆°i cho em h·ªèi", "Th·∫ßy c√¥ ∆°i cho em h·ªèi",
        "M√¨nh ƒëang quan t√¢m", "Em ƒëang t√¨m hi·ªÉu", "Cho m√¨nh tham kh·∫£o",
        "Em mu·ªën ƒë∆∞·ª£c t∆∞ v·∫•n", "M√¨nh c·∫ßn t∆∞ v·∫•n", "Cho em xin th√¥ng tin",
        "Em ƒëang ph√¢n v√¢n", "M√¨nh ƒëang c√¢n nh·∫Øc", "Em mu·ªën ƒë∆∞·ª£c gi·∫£i ƒë√°p th·∫Øc m·∫Øc"
    ],
    "teen": [
        "cho e h·ªèi", "cho mk h·ªèi", "cho t h·ªèi",
        "e mu·ªën bi·∫øt", "mk mu·ªën h·ªèi", "t mu·ªën h·ªèi",
        "ch·ªã ∆°i cho e h·ªèi x√≠u", "anh ∆°i cho e h·ªèi t√≠", "th·∫ßy c√¥ ∆°i cho e h·ªèi t√≠",
        "e ƒëang t√¨m hi·ªÉu", "mk ƒëang t√¨m hi·ªÉu", "cho e tham kh·∫£o t√≠",
        "e ƒëang ph√¢n v√¢n qu√°", "mk c·∫ßn t∆∞ v·∫•n t√≠", "cho e h√≥ng t√≠ info",
        "cho e xin √≠t th√¥ng tin", "e ƒëang bƒÉn khoƒÉn qu√°", "cho mk tham kh·∫£o t√≠"
    ],
    "typo": [
        "cho e hoi", "cho mk hoi", "cho t hoi",
        "e muon biet", "mk muon hoi", "t muon hoi",
        "chi oi cho e hoi xiu", "a oi cho e hoi ti", "thay co oi cho e hoi ti",
        "e dang tim hieu", "mk dang tim hieu", "cho e tham khao ti",
        "e dang phan van qua", "mk can tu van ti", "cho e hong ti info",
        "cho e xin it thong tin", "e dang ban khoan qua", "cho mk tham khao ti"
    ],
    "question_types": [
        "c√≥ th·ªÉ cho bi·∫øt", "l√†m ∆°n cho bi·∫øt", "gi√∫p m√¨nh",
        "cho m√¨nh/em bi·∫øt", "m√¨nh/em c·∫ßn bi·∫øt", "m√¨nh/em mu·ªën bi·∫øt",
        "ai bi·∫øt", "c√≥ ai bi·∫øt", "c√≥ ai r√µ",
        "l√†m sao", "nh∆∞ th·∫ø n√†o", "ra sao",
        "bao nhi√™u", "khi n√†o", "·ªü ƒë√¢u",
        "c√≥ ƒë∆∞·ª£c kh√¥ng", "c√≥ kh√≥ kh√¥ng", "c√≥ ph·∫£i kh√¥ng",
        "th·∫ø n√†o ·∫°", "th·∫ø n√†o nh·ªâ", "v·∫≠y ·∫°"
    ],
    "emotions": [
        "r·∫•t quan t√¢m v·ªÅ", "ƒëang bƒÉn khoƒÉn v·ªÅ", "ƒëang th·∫Øc m·∫Øc v·ªÅ",
        "r·∫•t mu·ªën bi·∫øt v·ªÅ", "r·∫•t c·∫ßn bi·∫øt v·ªÅ", "r·∫•t mong bi·∫øt v·ªÅ",
        "lo l·∫Øng v·ªÅ", "kh√¥ng r√µ v·ªÅ", "ch∆∞a hi·ªÉu v·ªÅ",
        "ƒëang ph√¢n v√¢n v·ªÅ", "ƒëang c√¢n nh·∫Øc v·ªÅ", "ƒëang do d·ª± v·ªÅ",
        "r·∫•t h·ª©ng th√∫ v·ªõi", "r·∫•t th√≠ch", "r·∫•t mu·ªën h·ªçc",
        "ƒëang mong mu·ªën ƒë∆∞·ª£c", "ƒëang hy v·ªçng ƒë∆∞·ª£c", "ƒëang m∆° ∆∞·ªõc ƒë∆∞·ª£c"
    ],
    "specific": [
        "cho h·ªèi c·ª• th·ªÉ v·ªÅ", "xin th√¥ng tin chi ti·∫øt v·ªÅ", "c·∫ßn t∆∞ v·∫•n k·ªπ v·ªÅ",
        "mu·ªën bi·∫øt r√µ h∆°n v·ªÅ", "c·∫ßn ƒë∆∞·ª£c t∆∞ v·∫•n k·ªπ c√†ng v·ªÅ", "xin ƒë∆∞·ª£c gi·∫£i th√≠ch r√µ v·ªÅ",
        "mong ƒë∆∞·ª£c t∆∞ v·∫•n c·ª• th·ªÉ v·ªÅ", "c·∫ßn th√¥ng tin chi ti·∫øt v·ªÅ", "mu·ªën hi·ªÉu r√µ h∆°n v·ªÅ"
    ]
}

# Define additional question patterns
additional_patterns = {
    "ask_program_fee": [
        "Chi ph√≠ m·ªôt nƒÉm h·ªçc {program_name} kho·∫£ng bao nhi√™u?",
        "H·ªçc ph√≠ {program_name} c√≥ ƒë·∫Øt kh√¥ng?",
        "Ngo√†i h·ªçc ph√≠, {program_name} c√≤n ph√≠ g√¨ kh√¥ng?",
        "C√≥ ƒë∆∞·ª£c gi·∫£m h·ªçc ph√≠ {program_name} kh√¥ng?",
        "H·ªçc ph√≠ {program_name} tr·∫£ theo k·ª≥ hay nƒÉm?",
        "So v·ªõi c√°c tr∆∞·ªùng kh√°c th√¨ h·ªçc ph√≠ {program_name} th·∫ø n√†o?",
        "H·ªçc ph√≠ ng√†nh {program_name} nƒÉm nay c√≥ tƒÉng kh√¥ng?",
        "M·ª©c h·ªçc ph√≠ c·ª• th·ªÉ c·ªßa ng√†nh {program_name} l√† bao nhi√™u?",
        "C√≥ ch√≠nh s√°ch h·ªó tr·ª£ h·ªçc ph√≠ cho ng√†nh {program_name} kh√¥ng?",
        "H·ªçc ph√≠ ng√†nh {program_name} c√≥ ƒë√≥ng theo t√≠n ch·ªâ kh√¥ng?",
        "Chi ph√≠ ∆∞·ªõc t√≠nh cho to√†n kh√≥a h·ªçc {program_name}?",
        "C√≥ ƒë∆∞·ª£c tr·∫£ g√≥p h·ªçc ph√≠ ng√†nh {program_name} kh√¥ng?"
    ],
    "ask_program_duration": [
        "Th·ªùi gian h·ªçc {program_name} c√≥ r√∫t ng·∫Øn ƒë∆∞·ª£c kh√¥ng?",
        "H·ªçc {program_name} c√≥ ƒë∆∞·ª£c h·ªçc nhanh h∆°n kh√¥ng?",
        "H·ªçc {program_name} m·∫•y h·ªçc k·ª≥ th√¨ xong?",
        "C√≥ th·ªÉ k√©o d√†i th·ªùi gian h·ªçc {program_name} kh√¥ng?",
        "H·ªçc {program_name} c√≥ h·ªçc k·ª≥ h√® kh√¥ng?",
        "Th·ªùi gian th·ª±c t·∫≠p {program_name} l√† bao l√¢u?",
        "Ng√†nh {program_name} h·ªçc m·∫•y nƒÉm th√¨ ra tr∆∞·ªùng?",
        "C√≥ th·ªÉ h·ªçc v∆∞·ª£t ng√†nh {program_name} kh√¥ng?",
        "Th·ªùi gian t·ªëi ƒëa ƒë∆∞·ª£c ph√©p h·ªçc ng√†nh {program_name}?",
        "H·ªçc {program_name} c√≥ ƒë∆∞·ª£c b·∫£o l∆∞u kh√¥ng?",
        "Ng√†nh {program_name} c√≥ ƒë∆∞·ª£c h·ªçc song song kh√¥ng?",
        "L·ªãch h·ªçc ng√†nh {program_name} nh∆∞ th·∫ø n√†o?"
    ],
    "ask_admission_score": [
        "ƒêi·ªÉm chu·∫©n {program_name} nƒÉm nay d·ª± ki·∫øn th·∫ø n√†o?",
        "ƒêi·ªÉm x√©t tuy·ªÉn {program_name} c√≥ tƒÉng kh√¥ng?",
        "ƒêi·ªÉm s√†n {program_name} l√† bao nhi√™u?",
        "ƒêi·ªÉm h·ªçc b·∫° {program_name} l·∫•y nh∆∞ th·∫ø n√†o?",
        "ƒêi·ªÉm x√©t tuy·ªÉn th·∫≥ng {program_name} th·∫ø n√†o?",
        "So v·ªõi nƒÉm tr∆∞·ªõc ƒëi·ªÉm {program_name} c√≥ kh√°c kh√¥ng?",
        "ƒêi·ªÉm chu·∫©n {program_name} nƒÉm {year} l√† bao nhi√™u?",
        "ƒêi·ªÉm tr√∫ng tuy·ªÉn {program_name} c√°c nƒÉm tr∆∞·ªõc?",
        "T·ªâ l·ªá ch·ªçi ng√†nh {program_name} nƒÉm nay?",
        "ƒêi·ªÉm x√©t tuy·ªÉn h·ªçc b·∫° {program_name} l·∫•y nh·ªØng m√¥n n√†o?",
        "ƒêi·ªÉm ∆∞u ti√™n khu v·ª±c cho ng√†nh {program_name}?",
        "ƒêi·ªÉm x√©t tuy·ªÉn {program_name} theo ph∆∞∆°ng th·ª©c {method_type}?"
    ],
    "ask_program_info": [
        "Sinh vi√™n {program_name} ƒë∆∞·ª£c h·ªçc nh·ªØng m√¥n g√¨?",
        "Sau khi t·ªët nghi·ªáp {program_name} l√†m vi·ªác ·ªü ƒë√¢u?",
        "Ng√†nh {program_name} c√≥ nhi·ªÅu vi·ªác l√†m kh√¥ng?",
        "M·ª©c l∆∞∆°ng c·ªßa sinh vi√™n {program_name} ra tr∆∞·ªùng?",
        "C√≥ n√™n h·ªçc ng√†nh {program_name} kh√¥ng?",
        "Ng√†nh {program_name} c√≥ ph√π h·ª£p v·ªõi n·ªØ kh√¥ng?",
        "Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ng√†nh {program_name} nh∆∞ th·∫ø n√†o?",
        "C∆° h·ªôi vi·ªác l√†m ng√†nh {program_name} ra sao?",
        "Ng√†nh {program_name} c√≥ th·ª±c t·∫≠p kh√¥ng?",
        "ƒêi·ªÉm m·∫°nh c·ªßa ng√†nh {program_name} CTU?",
        "Ng√†nh {program_name} c√≥ li√™n k·∫øt doanh nghi·ªáp kh√¥ng?",
        "Tri·ªÉn v·ªçng ngh·ªÅ nghi·ªáp ng√†nh {program_name} th·∫ø n√†o?"
    ],
    "ask_scholarship": [
        "H·ªçc b·ªïng {scholarship_type} c√≥ gi√° tr·ªã bao nhi√™u?",
        "ƒêi·ªÅu ki·ªán nh·∫≠n h·ªçc b·ªïng {scholarship_type}?",
        "Th·ªùi gian x√©t h·ªçc b·ªïng {scholarship_type}?",
        "C√≥ ƒë∆∞·ª£c nh·∫≠n nhi·ªÅu lo·∫°i h·ªçc b·ªïng c√πng l√∫c kh√¥ng?",
        "H·ªçc b·ªïng {scholarship_type} c√≥ duy tr√¨ ƒë∆∞·ª£c kh√¥ng?",
        "Quy tr√¨nh ƒëƒÉng k√Ω h·ªçc b·ªïng {scholarship_type}?",
        "S·ªë l∆∞·ª£ng su·∫•t h·ªçc b·ªïng {scholarship_type}?",
        "H·ªçc b·ªïng cho sinh vi√™n ng√†nh {program_name}?",
        "C√°c lo·∫°i h·ªçc b·ªïng d√†nh cho t√¢n sinh vi√™n?",
        "H·ªçc b·ªïng khuy·∫øn kh√≠ch h·ªçc t·∫≠p l√† g√¨?",
        "ƒêi·ªÉm trung b√¨nh ƒë·ªÉ ƒë∆∞·ª£c h·ªçc b·ªïng l√† bao nhi√™u?",
        "C√≥ h·ªçc b·ªïng cho sinh vi√™n ngh√®o v∆∞·ª£t kh√≥ kh√¥ng?"
    ],
    "ask_dormitory": [
        "K√Ω t√∫c x√° c√≥ wifi kh√¥ng?",
        "Gi√° ph√≤ng {dormitory_info} l√† bao nhi√™u?",
        "ƒêi·ªÅu ki·ªán ƒë·ªÉ ·ªü {dormitory_info}?",
        "Th·ªùi gian ƒëƒÉng k√Ω {dormitory_info}?",
        "C√≥ ƒë∆∞·ª£c n·∫•u ƒÉn trong {dormitory_info} kh√¥ng?",
        "Quy ƒë·ªãnh sinh ho·∫°t ·ªü {dormitory_info}?",
        "Ti·ªán √≠ch trong {dormitory_info} c√≥ nh·ªØng g√¨?",
        "C√≥ ph√≤ng m√°y l·∫°nh trong {dormitory_info} kh√¥ng?",
        "Th·ªß t·ª•c ƒëƒÉng k√Ω ·ªü {dormitory_info}?",
        "An ninh ·ªü {dormitory_info} th·∫ø n√†o?",
        "C√≥ gi·ªõi h·∫°n gi·ªù gi·∫•c ra v√†o kh√¥ng?",
        "C√≥ ƒë∆∞·ª£c ƒë·ªÉ xe trong {dormitory_info} kh√¥ng?"
    ],
    "ask_contact_info": [
        "S·ªë ƒëi·ªán tho·∫°i ph√≤ng ƒë√†o t·∫°o CTU?",
        "Email t∆∞ v·∫•n tuy·ªÉn sinh l√† g√¨?",
        "ƒê·ªãa ch·ªâ vƒÉn ph√≤ng tuy·ªÉn sinh ·ªü ƒë√¢u?",
        "Fanpage tuy·ªÉn sinh CTU?",
        "Zalo t∆∞ v·∫•n tuy·ªÉn sinh?",
        "Th·ªùi gian l√†m vi·ªác ph√≤ng tuy·ªÉn sinh?",
        "C√≥ t∆∞ v·∫•n tr·ª±c tuy·∫øn kh√¥ng?",
        "Hotline t∆∞ v·∫•n ng√†nh {program_name}?",
        "C√°ch li√™n h·ªá v·ªõi c·ªë v·∫•n h·ªçc t·∫≠p?",
        "Website tuy·ªÉn sinh ch√≠nh th·ª©c?",
        "ƒê·ªãa ch·ªâ n·ªôp h·ªì s∆° tr·ª±c ti·∫øp?",
        "K√™nh t∆∞ v·∫•n tuy·ªÉn sinh online?"
    ],
    "ask_campus_location": [
        "ƒê·ªãa ch·ªâ {campus_name} ·ªü ƒë√¢u?",
        "C√°ch ƒëi ƒë·∫øn {campus_name}?",
        "Kho·∫£ng c√°ch t·ª´ b·∫øn xe ƒë·∫øn {campus_name}?",
        "C√≥ xe bu√Ωt ƒë·∫øn {campus_name} kh√¥ng?",
        "B·∫£n ƒë·ªì ƒë∆∞·ªùng ƒëi {campus_name}?",
        "Ph∆∞∆°ng ti·ªán di chuy·ªÉn ƒë·∫øn {campus_name}?",
        "C√≥ k√Ω t√∫c x√° g·∫ßn {campus_name} kh√¥ng?",
        "Kho·∫£ng c√°ch gi·ªØa c√°c campus?",
        "M√¥i tr∆∞·ªùng h·ªçc t·∫≠p ·ªü {campus_name}?",
        "C∆° s·ªü v·∫≠t ch·∫•t {campus_name} th·∫ø n√†o?",
        "C√≥ cƒÉng tin ·ªü {campus_name} kh√¥ng?",
        "B√£i gi·ªØ xe ·ªü {campus_name}?"
    ]
}

# Define CTU-specific intent categories with enhanced diversity
CTU_INTENT_CATEGORIES = [
    {
        "intent_id": "ask_program_fee",
        "intent_name": "H·ªèi h·ªçc ph√≠ ng√†nh h·ªçc",
        "description": "C√¢u h·ªèi v·ªÅ h·ªçc ph√≠, chi ph√≠ ƒë√†o t·∫°o c·ªßa c√°c ng√†nh",
        "entities_required": ["program_name", "program_code"],
        "keywords": ["h·ªçc ph√≠", "chi ph√≠", "ti·ªÅn h·ªçc", "m·ª©c ph√≠", "ph√≠", "bao nhi√™u ti·ªÅn", "ƒë√≥ng h·ªçc ph√≠", "h·ªçc ph√≠ m·ªói k·ª≥", "h·ªçc ph√≠ m·ªói nƒÉm"],
        "seed_patterns": [
            "H·ªçc ph√≠ ng√†nh {program_name} l√† bao nhi√™u?",
            "M√£ ng√†nh {program_code} h·ªçc ph√≠ bao nhi√™u?",
            "Chi ph√≠ h·ªçc ng√†nh {program_name} CTU?",
            "H·ªçc ph√≠ m·ªói k·ª≥ c·ªßa ng√†nh {program_name} l√† bao nhi√™u?",
            "Ng√†nh {program_name} ƒë√≥ng h·ªçc ph√≠ nh∆∞ th·∫ø n√†o?",
            "H·ªçc ph√≠ ng√†nh {program_name} c√≥ tƒÉng theo nƒÉm kh√¥ng?",
            "C√≥ ƒë∆∞·ª£c mi·ªÖn gi·∫£m h·ªçc ph√≠ ng√†nh {program_name} kh√¥ng?",
            "H·ªçc ph√≠ ng√†nh {program_name} c√≥ kh√°c v·ªõi c√°c tr∆∞·ªùng kh√°c kh√¥ng?"
        ]
    },
    {
        "intent_id": "ask_program_duration",
        "intent_name": "H·ªèi th·ªùi gian ƒë√†o t·∫°o",
        "description": "C√¢u h·ªèi v·ªÅ th·ªùi gian h·ªçc, s·ªë nƒÉm ƒë√†o t·∫°o",
        "entities_required": ["program_name", "program_code"],
        "keywords": ["th·ªùi gian", "bao l√¢u", "m·∫•y nƒÉm", "h·ªçc m·∫•y nƒÉm", "th·ªùi gian ƒë√†o t·∫°o", "k·ª≥ h·ªçc", "h·ªçc k·ª≥", "t√≠n ch·ªâ"],
        "seed_patterns": [
            "Ng√†nh {program_name} h·ªçc m·∫•y nƒÉm?",
            "Th·ªùi gian ƒë√†o t·∫°o ng√†nh {program_name} l√† bao l√¢u?",
            "M√£ {program_code} h·ªçc trong bao nhi√™u nƒÉm?",
            "Ng√†nh {program_name} c√≥ bao nhi√™u t√≠n ch·ªâ?",
            "C√≥ th·ªÉ h·ªçc nhanh h∆°n ng√†nh {program_name} kh√¥ng?",
            "Ng√†nh {program_name} c√≥ h·ªçc k·ª≥ h√® kh√¥ng?",
            "Th·ªùi gian th·ª±c t·∫≠p ng√†nh {program_name} l√† bao l√¢u?",
            "C√≥ ƒë∆∞·ª£c h·ªçc v∆∞·ª£t ng√†nh {program_name} kh√¥ng?"
        ]
    },
    {
        "intent_id": "ask_admission_score",
        "intent_name": "H·ªèi ƒëi·ªÉm chu·∫©n",
        "description": "C√¢u h·ªèi v·ªÅ ƒëi·ªÉm chu·∫©n, ƒëi·ªÉm x√©t tuy·ªÉn c√°c nƒÉm",
        "entities_required": ["program_name", "program_code", "year"],
        "keywords": ["ƒëi·ªÉm chu·∫©n", "ƒëi·ªÉm x√©t tuy·ªÉn", "bao nhi√™u ƒëi·ªÉm", "ƒëi·ªÉm ƒë·∫ßu v√†o", "ƒëi·ªÉm tr√∫ng tuy·ªÉn", "ƒëi·ªÉm s√†n", "ƒëi·ªÉm li·ªát"],
        "seed_patterns": [
            "ƒêi·ªÉm chu·∫©n ng√†nh {program_name} nƒÉm {year} l√† bao nhi√™u?",
            "NƒÉm ngo√°i ng√†nh {program_name} l·∫•y bao nhi√™u ƒëi·ªÉm?",
            "ƒêi·ªÉm x√©t tuy·ªÉn m√£ {program_code} CTU?",
            "ƒêi·ªÉm s√†n ng√†nh {program_name} nƒÉm {year}?",
            "ƒêi·ªÉm li·ªát ng√†nh {program_name} l√† bao nhi√™u?",
            "ƒêi·ªÉm chu·∫©n ng√†nh {program_name} c√≥ tƒÉng kh√¥ng?",
            "ƒêi·ªÉm x√©t tuy·ªÉn h·ªçc b·∫° ng√†nh {program_name}?",
            "ƒêi·ªÉm chu·∫©n ng√†nh {program_name} theo t·ª´ng ph∆∞∆°ng th·ª©c?"
        ]
    },
    {
        "intent_id": "ask_admission_method",
        "intent_name": "H·ªèi ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn",
        "description": "C√¢u h·ªèi v·ªÅ c√°c ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn, c√°ch th·ª©c ƒëƒÉng k√Ω",
        "entities_required": ["program_name", "method_type"],
        "keywords": ["ph∆∞∆°ng th·ª©c", "x√©t tuy·ªÉn", "c√°ch x√©t", "h√¨nh th·ª©c", "x√©t h·ªçc b·∫°", "x√©t ƒëi·ªÉm thi", "ƒëƒÉng k√Ω", "h·ªì s∆°"],
        "seed_patterns": [
            "Ng√†nh {program_name} x√©t tuy·ªÉn b·∫±ng c√°ch n√†o?",
            "C√≥ th·ªÉ x√©t h·ªçc b·∫° v√†o ng√†nh {program_name} kh√¥ng?",
            "CTU c√≥ nh·ªØng ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn n√†o?",
            "ƒêi·ªÅu ki·ªán x√©t tuy·ªÉn ng√†nh {program_name}?",
            "H·ªì s∆° x√©t tuy·ªÉn ng√†nh {program_name} c·∫ßn nh·ªØng g√¨?",
            "Th·ªùi gian ƒëƒÉng k√Ω x√©t tuy·ªÉn ng√†nh {program_name}?",
            "C√≥ th·ªÉ ƒëƒÉng k√Ω nhi·ªÅu ph∆∞∆°ng th·ª©c ng√†nh {program_name} kh√¥ng?",
            "X√©t tuy·ªÉn th·∫≥ng ng√†nh {program_name} c·∫ßn ƒëi·ªÅu ki·ªán g√¨?"
        ]
    },
    {
        "intent_id": "ask_program_info",
        "intent_name": "H·ªèi th√¥ng tin ng√†nh h·ªçc",
        "description": "C√¢u h·ªèi chung v·ªÅ ng√†nh h·ªçc, c∆° h·ªôi vi·ªác l√†m",
        "entities_required": ["program_name"],
        "keywords": ["ng√†nh", "h·ªçc g√¨", "ra tr∆∞·ªùng", "l√†m g√¨", "c∆° h·ªôi", "ngh·ªÅ nghi·ªáp", "ch∆∞∆°ng tr√¨nh", "m√¥n h·ªçc"],
        "seed_patterns": [
            "Ng√†nh {program_name} h·ªçc nh·ªØng g√¨?",
            "Ra tr∆∞·ªùng ng√†nh {program_name} l√†m vi·ªác ·ªü ƒë√¢u?",
            "Gi·ªõi thi·ªáu v·ªÅ ng√†nh {program_name} CTU?",
            "Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o ng√†nh {program_name} nh∆∞ th·∫ø n√†o?",
            "C∆° h·ªôi vi·ªác l√†m ng√†nh {program_name} ra sao?",
            "Ng√†nh {program_name} c√≥ th·ª±c t·∫≠p kh√¥ng?",
            "ƒêi·ªÉm m·∫°nh c·ªßa ng√†nh {program_name} CTU?",
            "Ng√†nh {program_name} c√≥ li√™n k·∫øt doanh nghi·ªáp kh√¥ng?"
        ]
    },
    {
        "intent_id": "ask_scholarship",
        "intent_name": "H·ªèi h·ªçc b·ªïng",
        "description": "C√¢u h·ªèi v·ªÅ c√°c lo·∫°i h·ªçc b·ªïng, ƒëi·ªÅu ki·ªán nh·∫≠n h·ªçc b·ªïng",
        "entities_required": ["scholarship_type", "program_name"],
        "keywords": ["h·ªçc b·ªïng", "h·ªó tr·ª£", "mi·ªÖn gi·∫£m", "t√†i tr·ª£", "h·ªçc ph√≠", "sinh vi√™n", "ƒëi·ªÅu ki·ªán", "th·ªß t·ª•c"],
        "seed_patterns": [
            "CTU c√≥ nh·ªØng lo·∫°i h·ªçc b·ªïng n√†o?",
            "ƒêi·ªÅu ki·ªán nh·∫≠n h·ªçc b·ªïng ng√†nh {program_name}?",
            "L√†m sao ƒë·ªÉ ƒë∆∞·ª£c h·ªçc b·ªïng t·∫°i CTU?",
            "H·ªçc b·ªïng {scholarship_type} d√†nh cho ng√†nh {program_name}?",
            "Th·ªß t·ª•c xin h·ªçc b·ªïng ng√†nh {program_name}?",
            "H·ªçc b·ªïng c√≥ ƒë∆∞·ª£c c·∫•p l·∫°i kh√¥ng?",
            "H·ªçc b·ªïng ng√†nh {program_name} c√≥ kh√≥ kh√¥ng?",
            "C√≥ h·ªçc b·ªïng cho sinh vi√™n m·ªõi ng√†nh {program_name} kh√¥ng?"
        ]
    },
    {
        "intent_id": "ask_dormitory",
        "intent_name": "H·ªèi k√Ω t√∫c x√°",
        "description": "C√¢u h·ªèi v·ªÅ k√Ω t√∫c x√°, ch·ªó ·ªü cho sinh vi√™n",
        "entities_required": ["dormitory_info"],
        "keywords": ["k√Ω t√∫c x√°", "KTX", "ch·ªó ·ªü", "ph√≤ng ·ªü", "n·ªôi tr√∫", "ph√≤ng tr·ªç", "gi√° ph√≤ng", "ti·ªán nghi"],
        "seed_patterns": [
            "CTU c√≥ k√Ω t√∫c x√° kh√¥ng?",
            "Ph√≠ k√Ω t√∫c x√° CTU bao nhi√™u?",
            "ƒêƒÉng k√Ω KTX CTU nh∆∞ th·∫ø n√†o?",
            "Ti·ªán nghi {dormitory_info} c√≥ g√¨?",
            "Gi√° ph√≤ng {dormitory_info} l√† bao nhi√™u?",
            "C√≥ ph√≤ng cho sinh vi√™n n·ªØ kh√¥ng?",
            "KTX c√≥ wifi kh√¥ng?",
            "C√≥ ƒë∆∞·ª£c n·∫•u ƒÉn trong KTX kh√¥ng?"
        ]
    },
    {
        "intent_id": "ask_enrollment_process",
        "intent_name": "H·ªèi quy tr√¨nh nh·∫≠p h·ªçc",
        "description": "C√¢u h·ªèi v·ªÅ th·ªß t·ª•c nh·∫≠p h·ªçc, h·ªì s∆° c·∫ßn thi·∫øt",
        "entities_required": ["process_type"],
        "keywords": ["nh·∫≠p h·ªçc", "th·ªß t·ª•c", "h·ªì s∆°", "ƒëƒÉng k√Ω", "n·ªôp h·ªì s∆°", "gi·∫•y t·ªù", "th·ªùi gian", "ƒë·ªãa ƒëi·ªÉm"],
        "seed_patterns": [
            "Th·ªß t·ª•c nh·∫≠p h·ªçc CTU nh∆∞ th·∫ø n√†o?",
            "C·∫ßn chu·∫©n b·ªã h·ªì s∆° g√¨ ƒë·ªÉ nh·∫≠p h·ªçc?",
            "Khi n√†o nh·∫≠p h·ªçc t·∫°i CTU?",
            "ƒê·ªãa ƒëi·ªÉm n·ªôp h·ªì s∆° nh·∫≠p h·ªçc?",
            "C√≥ c·∫ßn gi·∫•y kh√°m s·ª©c kh·ªèe kh√¥ng?",
            "Th·ªùi gian l√†m th·ªß t·ª•c nh·∫≠p h·ªçc?",
            "C√≥ ƒë∆∞·ª£c ho√£n nh·∫≠p h·ªçc kh√¥ng?",
            "H·ªì s∆° nh·∫≠p h·ªçc c√≥ c·∫ßn c√¥ng ch·ª©ng kh√¥ng?"
        ]
    },
    {
        "intent_id": "ask_contact_info",
        "intent_name": "H·ªèi th√¥ng tin li√™n h·ªá",
        "description": "C√¢u h·ªèi v·ªÅ c√°ch li√™n h·ªá, t∆∞ v·∫•n tuy·ªÉn sinh",
        "entities_required": ["contact_type"],
        "keywords": ["li√™n h·ªá", "t∆∞ v·∫•n", "s·ªë ƒëi·ªán tho·∫°i", "email", "facebook", "ƒë·ªãa ch·ªâ", "hotline", "zalo"],
        "seed_patterns": [
            "S·ªë ƒëi·ªán tho·∫°i t∆∞ v·∫•n tuy·ªÉn sinh CTU?",
            "L√†m sao ƒë·ªÉ li√™n h·ªá t∆∞ v·∫•n CTU?",
            "Facebook tuy·ªÉn sinh CTU l√† g√¨?",
            "Email li√™n h·ªá ph√≤ng ƒë√†o t·∫°o?",
            "Hotline t∆∞ v·∫•n tuy·ªÉn sinh?",
            "Zalo t∆∞ v·∫•n tuy·ªÉn sinh CTU?",
            "Gi·ªù l√†m vi·ªác ph√≤ng tuy·ªÉn sinh?",
            "ƒê·ªãa ch·ªâ li√™n h·ªá tr·ª±c ti·∫øp?"
        ]
    },
    {
        "intent_id": "ask_campus_location",
        "intent_name": "H·ªèi v·ªã tr√≠ c∆° s·ªü",
        "description": "C√¢u h·ªèi v·ªÅ ƒë·ªãa ƒëi·ªÉm, c∆° s·ªü h·ªçc t·∫≠p",
        "entities_required": ["campus_name"],
        "keywords": ["ƒë·ªãa ch·ªâ", "·ªü ƒë√¢u", "c∆° s·ªü", "khu", "campus", "ƒë∆∞·ªùng", "ph∆∞·ªùng", "qu·∫≠n"],
        "seed_patterns": [
            "CTU c√≥ nh·ªØng c∆° s·ªü n√†o?",
            "ƒê·ªãa ch·ªâ CTU ·ªü ƒë√¢u?",
            "Khu {campus_name} CTU ·ªü ƒë√¢u?",
            "ƒê∆∞·ªùng ƒëi ƒë·∫øn {campus_name}?",
            "C√≥ xe bus ƒë·∫øn {campus_name} kh√¥ng?",
            "Kho·∫£ng c√°ch gi·ªØa c√°c c∆° s·ªü?",
            "B√£i ƒë·∫≠u xe ·ªü {campus_name}?",
            "C∆° s·ªü v·∫≠t ch·∫•t {campus_name} nh∆∞ th·∫ø n√†o?"
        ]
    },
    {
        "intent_id": "ask_transfer_program",
        "intent_name": "H·ªèi chuy·ªÉn ng√†nh",
        "description": "C√¢u h·ªèi v·ªÅ ƒëi·ªÅu ki·ªán v√† th·ªß t·ª•c chuy·ªÉn ng√†nh",
        "entities_required": ["program_name"],
        "keywords": ["chuy·ªÉn ng√†nh", "chuy·ªÉn tr∆∞·ªùng", "ƒëi·ªÅu ki·ªán", "th·ªß t·ª•c", "h·ªçc l·ª±c", "ƒëi·ªÉm", "th·ªùi gian"],
        "seed_patterns": [
            "ƒêi·ªÅu ki·ªán chuy·ªÉn ng√†nh {program_name}?",
            "C√≥ ƒë∆∞·ª£c chuy·ªÉn ng√†nh {program_name} kh√¥ng?",
            "Th·ªß t·ª•c chuy·ªÉn ng√†nh nh∆∞ th·∫ø n√†o?",
            "ƒêi·ªÉm trung b√¨nh ƒë·ªÉ chuy·ªÉn ng√†nh {program_name}?",
            "Th·ªùi gian ƒë∆∞·ª£c ph√©p chuy·ªÉn ng√†nh?",
            "Chuy·ªÉn ng√†nh c√≥ ph·∫£i thi l·∫°i kh√¥ng?",
            "H·ªçc ph√≠ khi chuy·ªÉn ng√†nh {program_name}?",
            "C√≥ ƒë∆∞·ª£c chuy·ªÉn ng√†nh nhi·ªÅu l·∫ßn kh√¥ng?"
        ]
    },
    {
        "intent_id": "ask_credit_transfer",
        "intent_name": "H·ªèi chuy·ªÉn t√≠n ch·ªâ",
        "description": "C√¢u h·ªèi v·ªÅ vi·ªác chuy·ªÉn ƒë·ªïi, c√¥ng nh·∫≠n t√≠n ch·ªâ",
        "entities_required": ["program_name"],
        "keywords": ["t√≠n ch·ªâ", "chuy·ªÉn ƒë·ªïi", "c√¥ng nh·∫≠n", "m√¥n h·ªçc", "ƒëi·ªÉm", "h·ªçc ph·∫ßn", "t∆∞∆°ng ƒë∆∞∆°ng"],
        "seed_patterns": [
            "C√≥ ƒë∆∞·ª£c chuy·ªÉn t√≠n ch·ªâ ng√†nh {program_name} kh√¥ng?",
            "ƒêi·ªÅu ki·ªán c√¥ng nh·∫≠n t√≠n ch·ªâ?",
            "M√¥n h·ªçc n√†o ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi t√≠n ch·ªâ?",
            "Th·ªß t·ª•c chuy·ªÉn t√≠n ch·ªâ nh∆∞ th·∫ø n√†o?",
            "ƒêi·ªÉm t√≠n ch·ªâ chuy·ªÉn ƒë·ªïi t√≠nh nh∆∞ th·∫ø n√†o?",
            "C√≥ gi·ªõi h·∫°n s·ªë t√≠n ch·ªâ chuy·ªÉn ƒë·ªïi kh√¥ng?",
            "Th·ªùi gian n·ªôp h·ªì s∆° chuy·ªÉn t√≠n ch·ªâ?",
            "T√≠n ch·ªâ t·ª´ tr∆∞·ªùng kh√°c c√≥ ƒë∆∞·ª£c c√¥ng nh·∫≠n kh√¥ng?"
        ]
    },
    {
        "intent_id": "ask_graduation_requirements",
        "intent_name": "H·ªèi ƒëi·ªÅu ki·ªán t·ªët nghi·ªáp",
        "description": "C√¢u h·ªèi v·ªÅ c√°c ƒëi·ªÅu ki·ªán ƒë·ªÉ t·ªët nghi·ªáp",
        "entities_required": ["program_name"],
        "keywords": ["t·ªët nghi·ªáp", "ƒëi·ªÅu ki·ªán", "y√™u c·∫ßu", "t√≠n ch·ªâ", "ƒëi·ªÉm", "kh√≥a lu·∫≠n", "ti·∫øng Anh", "ch·ª©ng ch·ªâ"],
        "seed_patterns": [
            "ƒêi·ªÅu ki·ªán t·ªët nghi·ªáp ng√†nh {program_name}?",
            "C·∫ßn bao nhi√™u t√≠n ch·ªâ ƒë·ªÉ t·ªët nghi·ªáp?",
            "Y√™u c·∫ßu ti·∫øng Anh ƒë·ªÉ t·ªët nghi·ªáp?",
            "C√≥ ph·∫£i l√†m kh√≥a lu·∫≠n kh√¥ng?",
            "ƒêi·ªÉm trung b√¨nh ƒë·ªÉ t·ªët nghi·ªáp?",
            "Th·ªùi gian t·ªëi ƒëa ƒë·ªÉ t·ªët nghi·ªáp?",
            "C√≥ ƒë∆∞·ª£c t·ªët nghi·ªáp s·ªõm kh√¥ng?",
            "Ch·ª©ng ch·ªâ c·∫ßn thi·∫øt ƒë·ªÉ t·ªët nghi·ªáp?"
        ]
    }
]


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def generate_intent_questions(intent_category: Dict, num_variations: int = 20) -> List[str]:
    """Generate question variations for a specific intent using OpenAI with retry logic"""
    
    client = openai.AsyncOpenAI(
        api_key=os.getenv('OPENAI_API_KEY'),
        timeout=30.0  # Set timeout to 30 seconds
    )
    
    # Get sample entity values for the prompt
    sample_values = {
        "program_name": [
            "C√¥ng ngh·ªá th√¥ng tin", "Y khoa", "D∆∞·ª£c h·ªçc", "Kinh t·∫ø", "Lu·∫≠t",
            "K·ªπ thu·∫≠t x√¢y d·ª±ng", "C√¥ng ngh·ªá sinh h·ªçc", "Ng√¥n ng·ªØ Anh",
            "C√¥ng ngh·ªá th√¥ng tin (CLC)", "Y khoa (CLC)", "D∆∞·ª£c h·ªçc (CLC)"
        ],
        "program_code": [
            "7480201", "7720101", "7720201", "7340101", "7380101",
            "7480201C", "7720101C", "7720201C"
        ],
        "year": ["2024", "2023", "2022", "nƒÉm ngo√°i", "nƒÉm nay", "nƒÉm sau"],
        "method_type": [
            "h·ªçc b·∫°", "ƒëi·ªÉm thi THPT", "x√©t tuy·ªÉn th·∫≥ng",
            "V-SAT", "thi nƒÉng l·ª±c", "x√©t tuy·ªÉn k·∫øt h·ª£p"
        ],
        "scholarship_type": [
            "khuy·∫øn kh√≠ch", "t√†i nƒÉng", "v∆∞·ª£t kh√≥",
            "doanh nghi·ªáp", "h·ªçc b·ªïng ch√≠nh ph·ªß"
        ],
        "dormitory_info": [
            "KTX A", "KTX B", "KTX C", "ph√≤ng 2 ng∆∞·ªùi",
            "ph√≤ng c√≥ ƒëi·ªÅu h√≤a", "ph√≤ng c√≥ wifi"
        ],
        "campus_name": [
            "Khu I", "Khu II", "Khu III", "Khu H√≤a An",
            "c∆° s·ªü ch√≠nh", "Xu√¢n Kh√°nh"
        ]
    }
    
    # Create example questions with real values
    example_questions = []
    
    # Add seed patterns
    if 'seed_patterns' in intent_category:
        for pattern in intent_category['seed_patterns']:
            filled_pattern = pattern
            for entity in intent_category['entities_required']:
                if entity in sample_values:
                    value = sample_values[entity][0]  # Use first sample value
                    filled_pattern = filled_pattern.replace(f"{{{entity}}}", value)
            example_questions.append(filled_pattern)
    
    # Add additional patterns if available
    if intent_category['intent_id'] in additional_patterns:
        for pattern in additional_patterns[intent_category['intent_id']]:
            filled_pattern = pattern
            for entity in intent_category['entities_required']:
                if entity in sample_values:
                    value = sample_values[entity][1]  # Use second sample value
                    filled_pattern = filled_pattern.replace(f"{{{entity}}}", value)
            example_questions.append(filled_pattern)
    
    prompt = f"""
B·∫°n l√† chuy√™n gia t·∫°o dataset c√¢u h·ªèi cho chatbot t∆∞ v·∫•n tuy·ªÉn sinh ƒê·∫°i h·ªçc C·∫ßn Th∆°.

INTENT: {intent_category['intent_name']}
M√î T·∫¢: {intent_category['description']}
T·ª™ KH√ìA: {', '.join(intent_category['keywords'])}
ENTITIES: {', '.join(intent_category['entities_required'])}

C√ÇU H·ªéI M·∫™U (ƒë√£ thay th·∫ø entity):
{chr(10).join(f"- {q}" for q in example_questions[:10])}  # Show first 10 examples

Y√äU C·∫¶U:
1. T·∫°o {num_variations} c√¢u h·ªèi KH√ÅC NHAU cho intent n√†y
2. S·ª≠ d·ª•ng ng√¥n ng·ªØ t·ª± nhi√™n, ƒëa d·∫°ng (formal, informal, vi·∫øt t·∫Øt)
3. Bao g·ªìm c·∫£ c√¢u h·ªèi c√≥ l·ªói ch√≠nh t·∫£ nh·∫π (realistic)
4. M·ªôt s·ªë c√¢u d√πng teen code, vi·∫øt t·∫Øt (e, m√¨nh, mk, bao nhiu)
5. ƒêa d·∫°ng c√°ch h·ªèi: tr·ª±c ti·∫øp, gi√°n ti·∫øp, l·ªãch s·ª±, th√¢n m·∫≠t
6. QUAN TR·ªåNG: S·ª≠ d·ª•ng c√°c gi√° tr·ªã th·ª±c t·∫ø cho entities, KH√îNG d√πng placeholder
7. Th√™m c·∫£m x√∫c v√† ng·ªØ c·∫£nh v√†o c√¢u h·ªèi (lo l·∫Øng, ph√¢n v√¢n, h·ª©ng th√∫)
8. S·ª≠ d·ª•ng t·ª´ ng·ªØ ph·ªï bi·∫øn trong gi·ªõi tr·∫ª v√† m·∫°ng x√£ h·ªôi
9. Th√™m c√°c chi ti·∫øt c·ª• th·ªÉ li√™n quan ƒë·∫øn CTU
10. ƒêa d·∫°ng ƒë·ªô d√†i c√¢u h·ªèi (ng·∫Øn, v·ª´a, d√†i)

V√ç D·ª§ GI√Å TR·ªä TH·ª∞C CHO ENTITIES:
{chr(10).join(f"- {entity}: {', '.join(values[:3])}" for entity, values in sample_values.items())}

V√ç D·ª§ NG√îN NG·ªÆ ƒêA D·∫†NG:
- Formal: "Xin cho bi·∫øt h·ªçc ph√≠ ng√†nh C√¥ng ngh·ªá th√¥ng tin l√† bao nhi√™u?"
- Informal: "cho e h·ªèi h·ªçc ph√≠ ng√†nh Y khoa bao nhi√™u ·∫°"
- Teen: "ng√†nh D∆∞·ª£c h·ªçc hp bao nhiu v·∫≠y ·∫°"
- Typo: "hoc phi nganh cntt la bao nhieu"
- Emotion: "em ƒëang r·∫•t lo l·∫Øng v·ªÅ h·ªçc ph√≠ ng√†nh Y khoa ·∫°"
- Context: "em ƒëang ph√¢n v√¢n gi·ªØa Y khoa CTU v√† tr∆∞·ªùng kh√°c, cho em h·ªèi h·ªçc ph√≠ ·∫°"
- Social: "cho mk h√≥ng x√≠u h·ªçc ph√≠ ng√†nh CNTT nha"
- Detail: "h·ªçc ph√≠ ng√†nh CNTT ch·∫•t l∆∞·ª£ng cao khu 2 l√† bao nhi√™u ·∫°"

TR·∫¢ L·ªúI:
B·∫°n PH·∫¢I tr·∫£ v·ªÅ m·ªôt m·∫£ng JSON ch·ª©a c√°c c√¢u h·ªèi HO√ÄN CH·ªàNH (ƒë√£ thay th·∫ø entity), v·ªõi format ch√≠nh x√°c nh∆∞ sau:
[
  "H·ªçc ph√≠ ng√†nh C√¥ng ngh·ªá th√¥ng tin l√† bao nhi√™u?",
  "M√£ ng√†nh 7480201 h·ªçc ph√≠ bao nhi√™u?",
  "Chi ph√≠ h·ªçc ng√†nh Y khoa CTU?"
]

L∆ØU √ù:
- M·ªói c√¢u h·ªèi ph·∫£i ƒë∆∞·ª£c ƒë·∫∑t trong d·∫•u ngo·∫∑c k√©p
- C√°c c√¢u h·ªèi ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y
- Kh√¥ng c√≥ text n√†o kh√°c ngo√†i m·∫£ng JSON
- Kh√¥ng ƒë∆∞·ª£c c√≥ d·∫•u ph·∫©y sau c√¢u h·ªèi cu·ªëi c√πng
- KH√îNG ƒë∆∞·ª£c ƒë·ªÉ placeholder nh∆∞ {{program_name}} trong c√¢u h·ªèi
"""

    try:
        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "B·∫°n l√† chuy√™n gia t·∫°o dataset c√¢u h·ªèi. B·∫°n CH·ªà ƒë∆∞·ª£c ph√©p tr·∫£ v·ªÅ m·ªôt m·∫£ng JSON ch·ª©a c√°c c√¢u h·ªèi HO√ÄN CH·ªàNH (ƒë√£ thay th·∫ø entity), kh√¥ng c√≥ text n√†o kh√°c."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8,  # Increased from 0.7 to 0.8 for more creativity
            max_tokens=2000
        )
        
        content = response.choices[0].message.content.strip()
        
        # Clean v√† parse JSON
        try:
            # Lo·∫°i b·ªè markdown code block n·∫øu c√≥
            if content.startswith('```'):
                content = content.split('```')[1]
                if content.startswith('json'):
                    content = content[4:]
            content = content.strip()
            
            # Try to fix common JSON issues
            content = content.replace('\n', ' ').replace('\r', '')
            content = content.replace('""', '"')  # Fix double quotes
            content = content.replace('\\"', '"')  # Fix escaped quotes
            
            # Parse JSON
            questions = json.loads(content)
            if isinstance(questions, list):
                # Validate each question is a string and doesn't contain placeholders
                valid_questions = []
                for q in questions:
                    if isinstance(q, str) and '{' not in q and '}' not in q:
                        valid_questions.append(q)
                    else:
                        print(f"Skipping invalid question: {q[:50]}...")
                
                if valid_questions:
                    return valid_questions
                else:
                    print("Error: No valid questions found")
                    return []
            else:
                print(f"Error: Expected list but got {type(questions)}")
                return []
                
        except json.JSONDecodeError as e:
            print(f"JSON parse error: {e}")
            print(f"Raw content: {content[:100]}...")
            return []
            
    except Exception as e:
        print(f"Error generating questions for {intent_category['intent_name']}: {e}")
        raise  # Re-raise for retry


async def enrich_with_entity_values(questions: List[str], intent_category: Dict) -> List[Dict]:
    """Replace entity placeholders with real values"""
    
    # Entity value mappings for CTU - expanded list
    entity_values = {
        "program_name": [
            # Ch∆∞∆°ng tr√¨nh ƒë·∫°i tr√†
            "C√¥ng ngh·ªá th√¥ng tin", "Y khoa", "D∆∞·ª£c h·ªçc", "Kinh t·∫ø",
            "Lu·∫≠t", "K·ªπ thu·∫≠t x√¢y d·ª±ng", "C√¥ng ngh·ªá sinh h·ªçc", 
            "Ng√¥n ng·ªØ Anh", "Qu·∫£n tr·ªã kinh doanh", "K·∫ø to√°n",
            "C√¥ng ngh·ªá th·ª±c ph·∫©m", "Th·ªßy s·∫£n", "N√¥ng nghi·ªáp",
            "C√¥ng ngh·ªá k·ªπ thu·∫≠t ƒëi·ªán", "C√¥ng ngh·ªá k·ªπ thu·∫≠t ƒëi·ªán t·ª≠",
            "C√¥ng ngh·ªá k·ªπ thu·∫≠t c∆° kh√≠", "C√¥ng ngh·ªá k·ªπ thu·∫≠t h√≥a h·ªçc",
            "C√¥ng ngh·ªá k·ªπ thu·∫≠t m√¥i tr∆∞·ªùng", "C√¥ng ngh·ªá k·ªπ thu·∫≠t x√¢y d·ª±ng",
            "C√¥ng ngh·ªá k·ªπ thu·∫≠t giao th√¥ng", "C√¥ng ngh·ªá k·ªπ thu·∫≠t c∆° ƒëi·ªán t·ª≠",
            "C√¥ng ngh·ªá k·ªπ thu·∫≠t ƒëi·ªÅu khi·ªÉn v√† t·ª± ƒë·ªông h√≥a", "C√¥ng ngh·ªá k·ªπ thu·∫≠t m√°y t√≠nh",
            # Ch∆∞∆°ng tr√¨nh ch·∫•t l∆∞·ª£ng cao v√† ti√™n ti·∫øn
            "C√¥ng ngh·ªá sinh h·ªçc (CTTT)", "Nu√¥i tr·ªìng th·ªßy s·∫£n (CTTT)",
            "Th√∫ y (CLC)", "C√¥ng ngh·ªá k·ªπ thu·∫≠t h√≥a h·ªçc (CLC)",
            "C√¥ng ngh·ªá th·ª±c ph·∫©m (CLC)", "K·ªπ thu·∫≠t x√¢y d·ª±ng (CLC)",
            "K·ªπ thu·∫≠t ƒëi·ªán (CLC)", "K·ªπ thu·∫≠t ƒëi·ªÅu khi·ªÉn v√† t·ª± ƒë·ªông h√≥a (CLC)",
            "C√¥ng ngh·ªá th√¥ng tin (CLC)", "K·ªπ thu·∫≠t ph·∫ßn m·ªÅm (CLC)",
            "M·∫°ng m√°y t√≠nh v√† truy·ªÅn th√¥ng d·ªØ li·ªáu (CLC)", "H·ªá th·ªëng th√¥ng tin (CLC)",
            "Qu·∫£n tr·ªã kinh doanh (CLC)"
        ],
        "program_code": [
            # Ch∆∞∆°ng tr√¨nh ƒë·∫°i tr√†
            "7480201",     # C√¥ng ngh·ªá th√¥ng tin
            "7720101",     # Y khoa
            "7720201",     # D∆∞·ª£c h·ªçc
            "7310101",     # Kinh t·∫ø
            "7380101",     # Lu·∫≠t
            "7580201",     # K·ªπ thu·∫≠t x√¢y d·ª±ng
            "7420201",     # C√¥ng ngh·ªá sinh h·ªçc
            "7220201",     # Ng√¥n ng·ªØ Anh
            "7340101",     # Qu·∫£n tr·ªã kinh doanh
            "7340301",     # K·∫ø to√°n
            "7540101",     # C√¥ng ngh·ªá th·ª±c ph·∫©m
            "7620301",     # Nu√¥i tr·ªìng th·ªßy s·∫£n
            "7620101",     # N√¥ng nghi·ªáp
            "7510301",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t ƒëi·ªán
            "7510302",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t ƒëi·ªán t·ª≠
            "7510201",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t c∆° kh√≠
            "7510401",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t h√≥a h·ªçc
            "7510406",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t m√¥i tr∆∞·ªùng
            "7510102",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t x√¢y d·ª±ng
            "7510104",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t giao th√¥ng
            "7510203",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t c∆° ƒëi·ªán t·ª≠
            "7510303",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t ƒëi·ªÅu khi·ªÉn v√† t·ª± ƒë·ªông h√≥a
            "7510103",     # C√¥ng ngh·ªá k·ªπ thu·∫≠t m√°y t√≠nh
            # Ch∆∞∆°ng tr√¨nh ch·∫•t l∆∞·ª£ng cao v√† ti√™n ti·∫øn
            "7420201T",    # C√¥ng ngh·ªá sinh h·ªçc (CTTT)
            "7620301T",    # Nu√¥i tr·ªìng th·ªßy s·∫£n (CTTT)
            "7640101C",    # Th√∫ y (CLC)
            "7510401C",    # C√¥ng ngh·ªá k·ªπ thu·∫≠t h√≥a h·ªçc (CLC)
            "7540101C",    # C√¥ng ngh·ªá th·ª±c ph·∫©m (CLC)
            "7580201C",    # K·ªπ thu·∫≠t x√¢y d·ª±ng (CLC)
            "7520201C",    # K·ªπ thu·∫≠t ƒëi·ªán (CLC)
            "7520216C",    # K·ªπ thu·∫≠t ƒëi·ªÅu khi·ªÉn v√† t·ª± ƒë·ªông h√≥a (CLC)
            "7480201C",    # C√¥ng ngh·ªá th√¥ng tin (CLC)
            "7480103C",    # K·ªπ thu·∫≠t ph·∫ßn m·ªÅm (CLC)
            "7480102C",    # M·∫°ng m√°y t√≠nh v√† truy·ªÅn th√¥ng d·ªØ li·ªáu (CLC)
            "7480104C",    # H·ªá th·ªëng th√¥ng tin (CLC)
            "7340101C"     # Qu·∫£n tr·ªã kinh doanh (CLC)
        ],
        "year": [
            "2020", "2021", "2022", "2023", "2024", "2025",
            "nƒÉm ngo√°i", "nƒÉm nay", "nƒÉm sau", "nƒÉm tr∆∞·ªõc",
            "2 nƒÉm tr∆∞·ªõc", "3 nƒÉm tr∆∞·ªõc", "4 nƒÉm tr∆∞·ªõc"
        ],
        "method_type": [
            "h·ªçc b·∫°", "ƒëi·ªÉm thi THPT", "V-SAT", "thi nƒÉng l·ª±c",
            "x√©t tuy·ªÉn th·∫≥ng", "∆∞u ti√™n x√©t tuy·ªÉn", "x√©t tuy·ªÉn h·ªçc b·∫°",
            "x√©t tuy·ªÉn k·∫øt h·ª£p", "x√©t tuy·ªÉn theo ch·ª©ng ch·ªâ qu·ªëc t·∫ø"
        ],
        "scholarship_type": [
            "khuy·∫øn kh√≠ch", "t√†i nƒÉng", "v∆∞·ª£t kh√≥", "doanh nghi·ªáp",
            "h·ªçc b·ªïng ch√≠nh ph·ªß", "h·ªçc b·ªïng n∆∞·ªõc ngo√†i", "h·ªçc b·ªïng ƒë·ªëi t√°c",
            "h·ªçc b·ªïng sinh vi√™n xu·∫•t s·∫Øc", "h·ªçc b·ªïng sinh vi√™n ngh√®o v∆∞·ª£t kh√≥"
        ],
        "dormitory_info": [
            "KTX A", "KTX B", "KTX C", "KTX D", "k√Ω t√∫c x√°",
            "ph√≤ng 2 ng∆∞·ªùi", "ph√≤ng 4 ng∆∞·ªùi", "ph√≤ng 6 ng∆∞·ªùi",
            "ph√≤ng nam", "ph√≤ng n·ªØ", "ph√≤ng c√≥ ƒëi·ªÅu h√≤a",
            "ph√≤ng c√≥ wifi", "ph√≤ng c√≥ nh√† b·∫øp", "ph√≤ng c√≥ ban c√¥ng"
        ],
        "campus_name": [
            "Khu I", "Khu II", "Khu III", "Khu H√≤a An",
            "c∆° s·ªü ch√≠nh", "Xu√¢n Kh√°nh", "Ninh Ki·ªÅu",
            "C√°i RƒÉng", "Th·ªët N·ªët", "Vƒ©nh Long"
        ],
        "faculty": [
            "Khoa CNTT", "Khoa Y", "Khoa D∆∞·ª£c", "Khoa Kinh t·∫ø",
            "Khoa Lu·∫≠t", "Khoa X√¢y d·ª±ng", "Khoa Sinh h·ªçc",
            "Khoa Ngo·∫°i ng·ªØ", "Khoa Qu·∫£n tr·ªã", "Khoa K·∫ø to√°n",
            "Khoa Th·ªßy s·∫£n", "Khoa N√¥ng nghi·ªáp", "Khoa C∆° kh√≠",
            "Khoa ƒêi·ªán", "Khoa ƒêi·ªán t·ª≠", "Khoa H√≥a h·ªçc",
            "Khoa M√¥i tr∆∞·ªùng", "Khoa Giao th√¥ng"
        ]
    }
    
    enriched_questions = []
    
    for question in questions:
        if '{' in question and '}' in question:
            import re
            placeholders = re.findall(r'\{(\w+)\}', question)
            
            # Increased variations from 3 to 10
            for i in range(min(10, len(entity_values.get(placeholders[0], [])))):
                filled_question = question
                for placeholder in placeholders:
                    if placeholder in entity_values and entity_values[placeholder]:
                        value = entity_values[placeholder][i % len(entity_values[placeholder])]
                        filled_question = filled_question.replace(f"{{{placeholder}}}", value)
                
                enriched_questions.append({
                    "text": filled_question,
                    "entities": placeholders,
                    "is_template": False,
                    "source": "generated"
                })
        else:
            enriched_questions.append({
                "text": question,
                "entities": [],
                "is_template": False,
                "source": "generated"
            })
    
    return enriched_questions


async def generate_intent_dataset():
    """Generate complete intent question dataset for CTU admission chatbot"""
    
    print("üöÄ Starting CTU Intent Question Generation...")
    
    dataset = IntentDataset(
        metadata={
            "version": "1.0",
            "created_date": datetime.now().isoformat(),
            "description": "Intent questions for CTU admission chatbot",
            "language": "vi",
            "domain": "university_admission"
        },
        intent_categories=[]
    )
    
    total_questions = 0
    
    for intent_config in CTU_INTENT_CATEGORIES:
        print(f"\nüìù Generating questions for: {intent_config['intent_name']}")
        
        try:
            # Generate variations in smaller batches
            all_questions = []
            batch_size = 100  # Increased from 50 to 100
            num_batches = 10  # Increased from 6 to 10 (1000 questions per intent)
            
            for batch in range(num_batches):
                print(f"   Generating batch {batch + 1}/{num_batches}...")
                batch_questions = await generate_intent_questions(intent_config, num_variations=batch_size)
                all_questions.extend(batch_questions)
                print(f"   Generated {len(batch_questions)} questions in this batch")
                
                # Add longer delay between batches to avoid rate limits
                if batch < num_batches - 1:
                    await asyncio.sleep(5)  # Increased from 2 to 5 seconds
            
            print(f"   Generated {len(all_questions)} raw questions")
            
            # Enrich with entity values
            enriched_questions = await enrich_with_entity_values(
                intent_config['seed_patterns'] + all_questions,
                intent_config
            )
            print(f"   Enriched to {len(enriched_questions)} questions")
            
            # Create IntentCategory object
            intent_category = IntentCategory(
                intent_id=intent_config['intent_id'],
                intent_name=intent_config['intent_name'],
                description=intent_config['description'],
                entities_required=intent_config['entities_required'],
                keywords=intent_config['keywords'],
                questions=[IntentQuestion(**q) for q in enriched_questions]
            )
            
            dataset.intent_categories.append(intent_category)
            total_questions += len(enriched_questions)
            
            # Save intermediate results
            output_dir = Path("output/intent_dataset")
            output_dir.mkdir(exist_ok=True, parents=True)
            
            intermediate_file = output_dir / f"ctu_intent_questions_{intent_config['intent_id']}.json"
            with open(intermediate_file, "w", encoding="utf-8") as f:
                json.dump(intent_category.model_dump(), f, ensure_ascii=False, indent=2)
            
            print(f"   Saved intermediate results to: {intermediate_file}")
            
        except Exception as e:
            print(f"Error processing intent {intent_config['intent_name']}: {e}")
            continue
    
    dataset.total_questions = total_questions
    
    # Save final dataset
    output_dir = Path("output/intent_dataset")
    output_file = output_dir / "ctu_intent_questions.json"
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(dataset.model_dump(), f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ DATASET GENERATION COMPLETE!")
    print(f"üìä Statistics:")
    print(f"   - Total intents: {len(dataset.intent_categories)}")
    print(f"   - Total questions: {total_questions}")
    print(f"   - Average per intent: {total_questions / len(dataset.intent_categories):.1f}")
    print(f"   - Saved to: {output_file}")
    
    # Generate summary report
    summary = {
        "generation_date": datetime.now().isoformat(),
        "statistics": {
            "total_intents": len(dataset.intent_categories),
            "total_questions": total_questions,
            "by_intent": {}
        }
    }
    
    for category in dataset.intent_categories:
        summary["statistics"]["by_intent"][category.intent_id] = {
            "name": category.intent_name,
            "question_count": len(category.questions),
            "entities": category.entities_required,
            "keywords": category.keywords
        }
    
    summary_file = output_dir / "generation_summary.json"
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"   - Summary saved to: {summary_file}")
    
    return dataset


async def validate_dataset(dataset_file: str):
    """Validate generated dataset for quality and coverage"""
    
    print("\nüîç Validating dataset...")
    
    with open(dataset_file, "r", encoding="utf-8") as f:
        dataset = json.load(f)
    
    issues = []
    
    # Check for duplicate questions
    all_questions = []
    for category in dataset["intent_categories"]:
        for q in category["questions"]:
            if q["text"].lower() in [x.lower() for x in all_questions]:
                issues.append(f"Duplicate: {q['text']}")
            all_questions.append(q["text"])
    
    # Check question length variety
    lengths = [len(q) for q in all_questions]
    avg_length = sum(lengths) / len(lengths)
    
    print(f"\nüìä Validation Results:")
    print(f"   - Total questions: {len(all_questions)}")
    print(f"   - Unique questions: {len(set(q.lower() for q in all_questions))}")
    print(f"   - Average length: {avg_length:.1f} chars")
    print(f"   - Min/Max length: {min(lengths)}/{max(lengths)} chars")
    
    if issues:
        print(f"\n‚ö†Ô∏è  Found {len(issues)} issues:")
        for issue in issues[:5]:  # Show first 5
            print(f"   - {issue}")
    else:
        print("\n‚úÖ No issues found!")


def normalize_text(text: str) -> str:
    """Normalize text for comparison to avoid duplicates"""
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Remove extra spaces
    text = ' '.join(text.split())
    # Remove common variations
    text = text.replace('ctu', 'ƒë·∫°i h·ªçc c·∫ßn th∆°')
    text = text.replace('ƒëhct', 'ƒë·∫°i h·ªçc c·∫ßn th∆°')
    text = text.replace('ƒë·∫°i h·ªçc c·∫ßn th∆°', '')
    return text

def is_similar(text1: str, text2: str, threshold: float = 0.85) -> bool:
    """Check if two texts are similar using sequence matcher"""
    text1 = normalize_text(text1)
    text2 = normalize_text(text2)
    return SequenceMatcher(None, text1, text2).ratio() > threshold

def merge_datasets(existing_file: str, new_file: str, output_file: str):
    """Merge two datasets while avoiding duplicates"""
    print(f"\nüîÑ Merging datasets...")
    
    # Load existing dataset
    with open(existing_file, 'r', encoding='utf-8') as f:
        existing = json.load(f)
    
    # Load new dataset
    with open(new_file, 'r', encoding='utf-8') as f:
        new = json.load(f)
    
    # Track unique questions
    unique_questions: Dict[str, Set[str]] = {}
    for intent in existing['intent_categories']:
        unique_questions[intent['intent_id']] = {
            normalize_text(q['text']) for q in intent['questions']
        }
    
    # Merge datasets
    merged = existing.copy()
    total_added = 0
    total_skipped = 0
    
    for new_intent in new['intent_categories']:
        intent_id = new_intent['intent_id']
        
        # Find matching intent in existing dataset
        existing_intent = next(
            (i for i in merged['intent_categories'] if i['intent_id'] == intent_id),
            None
        )
        
        if existing_intent:
            # Add new questions to existing intent
            for question in new_intent['questions']:
                normalized = normalize_text(question['text'])
                
                # Check for duplicates
                is_duplicate = False
                for existing_norm in unique_questions[intent_id]:
                    if is_similar(normalized, existing_norm):
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    existing_intent['questions'].append(question)
                    unique_questions[intent_id].add(normalized)
                    total_added += 1
                else:
                    total_skipped += 1
        else:
            # Add new intent
            merged['intent_categories'].append(new_intent)
            unique_questions[intent_id] = {
                normalize_text(q['text']) for q in new_intent['questions']
            }
            total_added += len(new_intent['questions'])
    
    # Update total questions
    merged['total_questions'] = sum(
        len(intent['questions']) for intent in merged['intent_categories']
    )
    
    # Save merged dataset
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ Merge complete!")
    print(f"üìä Statistics:")
    print(f"   - Questions added: {total_added}")
    print(f"   - Questions skipped (duplicates): {total_skipped}")
    print(f"   - Total questions: {merged['total_questions']}")
    print(f"   - Saved to: {output_file}")
    
    return merged

async def generate_more_questions(
    intent_category: Dict,
    num_variations: int = 20,
    style_variations: List[str] = None
) -> List[str]:
    """Generate additional question variations with different styles"""
    
    if style_variations is None:
        style_variations = list(style_patterns.keys())
    
    # Get base questions from seed patterns and additional patterns
    base_questions = []
    
    # Add seed patterns
    if 'seed_patterns' in intent_category:
        base_questions.extend(intent_category['seed_patterns'])
    
    # Add additional patterns if available
    if intent_category['intent_id'] in additional_patterns:
        base_questions.extend(additional_patterns[intent_category['intent_id']])
    
    # Add some basic questions from keywords if no patterns
    if not base_questions and 'keywords' in intent_category:
        for keyword in intent_category['keywords']:
            base_questions.append(f"{keyword} ng√†nh {{program_name}}?")
            base_questions.append(f"Th√¥ng tin v·ªÅ {keyword} ng√†nh {{program_name}}?")
            base_questions.append(f"Cho h·ªèi {keyword} ng√†nh {{program_name}}?")
    
    # Replace entities with sample values
    filled_questions = []
    for question in base_questions:
        # Try multiple combinations of entity values
        for i in range(3):  # Generate 3 variations with different values
            filled = question
            for entity in intent_category.get('entities_required', []):
                if entity == 'program_name':
                    values = ["C√¥ng ngh·ªá th√¥ng tin", "Y khoa", "D∆∞·ª£c h·ªçc", "Kinh t·∫ø", "Lu·∫≠t"]
                    filled = filled.replace(f"{{{entity}}}", values[i % len(values)])
                elif entity == 'program_code':
                    values = ["7480201", "7720101", "7720201", "7340101", "7380101"]
                    filled = filled.replace(f"{{{entity}}}", values[i % len(values)])
                elif entity == 'year':
                    values = ["2024", "2023", "2022"]
                    filled = filled.replace(f"{{{entity}}}", values[i % len(values)])
                elif entity == 'method_type':
                    values = ["h·ªçc b·∫°", "ƒëi·ªÉm thi THPT", "x√©t tuy·ªÉn th·∫≥ng"]
                    filled = filled.replace(f"{{{entity}}}", values[i % len(values)])
                else:
                    filled = filled.replace(f"{{{entity}}}", f"gi√° tr·ªã {entity}")
            filled_questions.append(filled)
    
    # Add style variations
    all_variations = []
    for question in filled_questions:
        # Keep original
        all_variations.append(question)
        
        # Add style variations
        for style in style_variations:
            if style in style_patterns:
                for prefix in style_patterns[style]:
                    # Add prefix if not already similar
                    if not any(is_similar(f"{prefix} {question.lower()}", v) for v in all_variations):
                        variation = f"{prefix} {question.lower()}"
                        all_variations.append(variation)
                        
                        # Add question mark if missing
                        if not variation.endswith('?'):
                            all_variations.append(f"{variation}?")
    
    # Remove duplicates while preserving order
    seen = set()
    unique_variations = []
    for v in all_variations:
        normalized = normalize_text(v)
        if normalized not in seen:
            seen.add(normalized)
            unique_variations.append(v)
    
    return unique_variations[:num_variations]  # Return only requested number of variations

async def enrich_dataset(
    input_file: str,
    output_file: str,
    questions_per_intent: int = 100
):
    """Enrich existing dataset with more variations"""
    
    print(f"\nüîÑ Enriching dataset...")
    
    # Load existing dataset
    with open(input_file, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    # Add seed patterns if missing
    for intent in dataset['intent_categories']:
        if 'seed_patterns' not in intent:
            # Get seed patterns from CTU_INTENT_CATEGORIES
            matching_intent = next(
                (i for i in CTU_INTENT_CATEGORIES if i['intent_id'] == intent['intent_id']),
                None
            )
            if matching_intent:
                intent['seed_patterns'] = matching_intent['seed_patterns']
            else:
                # Generate basic seed patterns from keywords
                intent['seed_patterns'] = [
                    f"{keyword} ng√†nh {{program_name}}?" 
                    for keyword in intent['keywords'][:3]  # Use first 3 keywords
                ]
                # Add some common patterns
                intent['seed_patterns'].extend([
                    f"Cho em h·ªèi v·ªÅ {intent['intent_name'].lower()}?",
                    f"Th√¥ng tin v·ªÅ {intent['intent_name'].lower()}?",
                    f"ƒêi·ªÅu ki·ªán {intent['intent_name'].lower()}?"
                ])
    
    # Track unique questions
    unique_questions: Dict[str, Set[str]] = {}
    for intent in dataset['intent_categories']:
        unique_questions[intent['intent_id']] = {
            normalize_text(q['text']) for q in intent['questions']
        }
    
    # Generate more questions for each intent
    total_added = 0
    for intent in dataset['intent_categories']:
        print(f"\nüìù Enriching intent: {intent['intent_name']}")
        
        # Calculate how many more questions needed
        current_count = len(intent['questions'])
        needed = max(0, questions_per_intent - current_count)
        
        if needed > 0:
            try:
                # Generate more questions
                new_questions = await generate_more_questions(
                    intent,
                    num_variations=min(needed // 2, 20),  # Limit to 20 variations per batch
                    style_variations=["formal", "informal", "teen", "typo"]
                )
                
                # Add non-duplicate questions
                added = 0
                for question in new_questions:
                    normalized = normalize_text(question)
                    
                    # Check for duplicates
                    is_duplicate = False
                    for existing_norm in unique_questions[intent['intent_id']]:
                        if is_similar(normalized, existing_norm):
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        intent['questions'].append({
                            "text": question,
                            "entities": intent.get('entities_required', []),
                            "is_template": False,
                            "source": "enriched"
                        })
                        unique_questions[intent['intent_id']].add(normalized)
                        added += 1
                
                total_added += added
                print(f"   - Added {added} new questions")
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error generating questions for {intent['intent_name']}: {e}")
                continue
    
    # Update total questions
    dataset['total_questions'] = sum(
        len(intent['questions']) for intent in dataset['intent_categories']
    )
    
    # Save enriched dataset
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ Enrichment complete!")
    print(f"üìä Statistics:")
    print(f"   - Total questions added: {total_added}")
    print(f"   - New total questions: {dataset['total_questions']}")
    print(f"   - Saved to: {output_file}")
    
    return dataset

async def main():
    """Main function to generate and manage intent dataset"""
    
    # Load environment variables
    load_dotenv(override=True)
    
    # Check API key
    if not os.getenv('OPENAI_API_KEY'):
        print("‚ùå Please set OPENAI_API_KEY in .env file")
        return
    
    output_dir = Path("output/intent_dataset")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # Check if dataset exists
    existing_file = output_dir / "ctu_intent_questions.json"
    if existing_file.exists():
        # Ask user what to do
        print("\nüìÇ Found existing dataset!")
        print("What would you like to do?")
        print("1. Generate new dataset")
        print("2. Enrich existing dataset")
        print("3. Merge with new dataset")
        
        choice = input("\nEnter your choice (1-3): ").strip()
        
        if choice == "1":
            # Generate new dataset
            dataset = await generate_intent_dataset()
            await validate_dataset(str(existing_file))
            
        elif choice == "2":
            # Enrich existing dataset
            enriched_file = output_dir / "ctu_intent_questions_enriched.json"
            dataset = await enrich_dataset(
                str(existing_file),
                str(enriched_file),
                questions_per_intent=500  # Target 500 questions per intent
            )
            await validate_dataset(str(enriched_file))
            
        elif choice == "3":
            # Generate new and merge
            new_file = output_dir / "ctu_intent_questions_new.json"
            dataset = await generate_intent_dataset()
            merged_file = output_dir / "ctu_intent_questions_merged.json"
            dataset = merge_datasets(
                str(existing_file),
                str(new_file),
                str(merged_file)
            )
            await validate_dataset(str(merged_file))
            
        else:
            print("‚ùå Invalid choice!")
            return
            
    else:
        # Generate new dataset
        dataset = await generate_intent_dataset()
        await validate_dataset(str(existing_file))
    
    print("\nüéâ Dataset management completed!")

if __name__ == "__main__":
    asyncio.run(main()) 
import asyncio
import json
import os
from pathlib import Path
from datetime import datetime

from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from dotenv import load_dotenv

# Danh s√°ch URLs v√† categories c·∫ßn crawl
CRAWL_TARGETS = [
    {
        "url": "https://tuyensinh.ctu.edu.vn/",
        "category": "tuyen_sinh_general",
        "description": "Th√¥ng tin tuy·ªÉn sinh t·ªïng qu√°t",
        "prompt": "Extract general admission information, overview, and basic Q&A pairs in Vietnamese"
    },
    {
        "url": "https://tuyensinh.ctu.edu.vn/danh-muc-nganh-va-chi-tieu-tuyen-sinh",
        "category": "nganh_hoc", 
        "description": "Danh s√°ch ng√†nh v√† ch·ªâ ti√™u",
        "prompt": "Extract major names, codes, quotas, and related Q&A pairs in Vietnamese"
    },
    {
        "url": "https://tuyensinh.ctu.edu.vn/phuong-thuc-xet-tuyen",
        "category": "phuong_thuc_xet_tuyen",
        "description": "Ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn",
        "prompt": "Extract admission methods, requirements, deadlines, and related Q&A pairs in Vietnamese"
    },
    {
        "url": "https://tuyensinh.ctu.edu.vn/hoc-phi",
        "category": "hoc_phi",
        "description": "Th√¥ng tin h·ªçc ph√≠",
        "prompt": "Extract tuition fees, payment methods, scholarships, and related Q&A pairs in Vietnamese"
    },
    {
        "url": "https://tuyensinh.ctu.edu.vn/lien-he",
        "category": "lien_he",
        "description": "Th√¥ng tin li√™n h·ªá",
        "prompt": "Extract contact information, office hours, locations, and related Q&A pairs in Vietnamese"
    }
]

async def crawl_single_category(crawler, target, api_key, output_dir):
    """
    Crawl m·ªôt category ri√™ng bi·ªát
    """
    print(f"\nüîç ƒêang crawl: {target['description']}")
    print(f"üìç URL: {target['url']}")
    
    # T·∫°o prompt c·ª• th·ªÉ cho category
    instruction = f"""
    You are extracting data for category: {target['category']}
    
    {target['prompt']}
    
    Focus on:
    - Creating natural Vietnamese Q&A pairs
    - Extracting specific information for this category
    - Maintaining accuracy and completeness
    - Using proper Vietnamese language
    
    Return structured data with Q&A pairs and relevant information.
    """
    
    # Schema ƒë∆°n gi·∫£n cho t·ª´ng category
    schema = {
        "type": "object",
        "properties": {
            "category": {"type": "string"},
            "qa_pairs": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "question": {"type": "string"},
                        "answer": {"type": "string"},
                        "priority": {"type": "integer"}
                    }
                }
            },
            "structured_data": {"type": "object"},
            "metadata": {
                "type": "object",
                "properties": {
                    "url": {"type": "string"},
                    "category": {"type": "string"},
                    "crawl_date": {"type": "string"}
                }
            }
        }
    }
    
    # Configure LLM strategy
    llm_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(
            provider="openai/gpt-4o-mini",
            api_token=api_key
        ),
        schema=schema,
        extraction_type="schema",
        instruction=instruction,
        chunk_token_threshold=1000,
        overlap_rate=0.1,
        apply_chunking=True,
        input_format="markdown",
        extra_args={"temperature": 0.0, "max_tokens": 3000}
    )
    
    # Configure crawler run
    run_config = CrawlerRunConfig(
        extraction_strategy=llm_strategy,
        cache_mode=CacheMode.BYPASS
    )
    
    try:
        # Run crawler
        result = await crawler.arun(url=target['url'], config=run_config)
        
        if result.success and result.extracted_content:
            # Parse extracted data
            extracted_data = json.loads(result.extracted_content) if isinstance(result.extracted_content, str) else result.extracted_content
            
            # Add metadata
            if isinstance(extracted_data, dict):
                extracted_data['metadata'] = {
                    'url': target['url'],
                    'category': target['category'],
                    'description': target['description'],
                    'crawl_date': datetime.now().isoformat(),
                    'success': True
                }
            
            # Save to category-specific file
            output_file = Path(output_dir) / f"{target['category']}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, indent=2, ensure_ascii=False)
            
            # Save raw markdown
            markdown_file = Path(output_dir) / f"{target['category']}.md"
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(result.markdown.raw_markdown if hasattr(result, 'markdown') and result.markdown else "")
            
            print(f"‚úÖ Th√†nh c√¥ng: {output_file}")
            return extracted_data
            
        else:
            print(f"‚ùå L·ªói crawl: {result.error_message if hasattr(result, 'error_message') else 'Unknown error'}")
            return None
            
    except Exception as e:
        print(f"‚ùå Exception: {str(e)}")
        return None

async def main():
    """
    Main function ƒë·ªÉ crawl t·∫•t c·∫£ categories
    """
    # Load environment
    load_dotenv(override=True)
    api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        print("‚ùå Kh√¥ng t√¨m th·∫•y OPENAI_API_KEY!")
        return
    
    # Setup output directory
    output_dir = "output/categories"
    os.makedirs(output_dir, exist_ok=True)
    
    # Browser config
    browser_config = BrowserConfig(
        headless=True,
        verbose=False
    )
    
    print("üöÄ B·∫Øt ƒë·∫ßu crawl theo categories...")
    print(f"üìÅ Output directory: {output_dir}")
    
    results = {}
    
    # Crawl each category
    async with AsyncWebCrawler(config=browser_config) as crawler:
        for target in CRAWL_TARGETS:
            result = await crawl_single_category(crawler, target, api_key, output_dir)
            results[target['category']] = result
            
            # Delay between requests
            await asyncio.sleep(2)
    
    # Create summary
    summary = {
        "crawl_summary": {
            "total_categories": len(CRAWL_TARGETS),
            "successful_crawls": len([r for r in results.values() if r is not None]),
            "failed_crawls": len([r for r in results.values() if r is None]),
            "crawl_date": datetime.now().isoformat(),
            "categories": list(results.keys())
        },
        "results": results
    }
    
    # Save summary
    summary_file = Path(output_dir) / "crawl_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nüìä T·ªïng k·∫øt:")
    print(f"   ‚úÖ Th√†nh c√¥ng: {summary['crawl_summary']['successful_crawls']}")
    print(f"   ‚ùå Th·∫•t b·∫°i: {summary['crawl_summary']['failed_crawls']}")
    print(f"   üìÑ Summary: {summary_file}")

if __name__ == "__main__":
    asyncio.run(main()) 
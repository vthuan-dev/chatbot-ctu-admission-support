import asyncio
import json
import os
from pathlib import Path
from datetime import datetime

from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from dotenv import load_dotenv

def load_crawled_content(md_file):
    """
    Load content from crawled markdown file
    """
    with open(md_file, 'r', encoding='utf-8') as f:
        content = f.read()
    return content

async def extract_qa_from_content(content, api_key, output_file):
    """
    Extract Q&A pairs from crawled content using LLM
    """
    print(f"üîç Extracting Q&A from content ({len(content)} characters)")
    
    # Create comprehensive prompt for CTU admission data
    instruction = """
    B·∫°n l√† chuy√™n gia t∆∞ v·∫•n tuy·ªÉn sinh ƒê·∫°i h·ªçc C·∫ßn Th∆° (CTU). 
    H√£y ph√¢n t√≠ch n·ªôi dung v√† t·∫°o ra c√°c c·∫∑p h·ªèi-ƒë√°p ti·∫øng Vi·ªát t·ª± nhi√™n m√† sinh vi√™n th∆∞·ªùng h·ªèi.
    
    QUAN TR·ªåNG:
    - T·∫•t c·∫£ c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi ph·∫£i b·∫±ng ti·∫øng Vi·ªát
    - T·∫°o c√¢u h·ªèi t·ª± nhi√™n nh∆∞ sinh vi√™n th·∫≠t s·ª± h·ªèi
    - Tr·∫£ l·ªùi chi ti·∫øt, ch√≠nh x√°c d·ª±a tr√™n n·ªôi dung
    - Bao g·ªìm m√£ ng√†nh, ch·ªâ ti√™u, h·ªçc ph√≠, t·ªï h·ª£p x√©t tuy·ªÉn
    - T·∫°o nhi·ªÅu c√¢u h·ªèi kh√°c nhau cho c√πng m·ªôt th√¥ng tin
    
    V√≠ d·ª• c√¢u h·ªèi t·ªët:
    - "Ng√†nh C√¥ng ngh·ªá th√¥ng tin c√≥ m√£ ng√†nh g√¨?"
    - "H·ªçc ph√≠ ng√†nh CNTT ch·∫•t l∆∞·ª£ng cao bao nhi√™u?"
    - "T·ªï h·ª£p x√©t tuy·ªÉn ng√†nh K·ªπ thu·∫≠t ph·∫ßn m·ªÅm l√† g√¨?"
    - "Ch·ªâ ti√™u tuy·ªÉn sinh ng√†nh Th√∫ y nƒÉm 2025?"
    - "Kh√°c bi·ªát gi·ªØa ch∆∞∆°ng tr√¨nh ti√™n ti·∫øn v√† ch·∫•t l∆∞·ª£ng cao?"
    
    Ph√¢n lo·∫°i theo priority:
    1 = Th√¥ng tin c∆° b·∫£n (m√£ ng√†nh, ch·ªâ ti√™u)
    2 = Th√¥ng tin quan tr·ªçng (h·ªçc ph√≠, t·ªï h·ª£p)  
    3 = Th√¥ng tin chi ti·∫øt (chuy√™n ng√†nh, ghi ch√∫)
    """
    
    # Schema for Q&A extraction
    schema = {
        "type": "object",
        "properties": {
            "qa_pairs": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "question": {"type": "string"},
                        "answer": {"type": "string"},
                        "category": {"type": "string"},
                        "priority": {"type": "integer", "minimum": 1, "maximum": 3},
                        "entities": {
                            "type": "object",
                            "properties": {
                                "ma_nganh": {"type": "string"},
                                "ten_nganh": {"type": "string"},
                                "chi_tieu": {"type": "integer"},
                                "hoc_phi": {"type": "string"},
                                "to_hop": {"type": "array", "items": {"type": "string"}},
                                "chuong_trinh": {"type": "string"}
                            }
                        }
                    },
                    "required": ["question", "answer", "category", "priority"]
                }
            },
            "summary": {
                "type": "object",
                "properties": {
                    "total_majors": {"type": "integer"},
                    "total_quota": {"type": "integer"},
                    "special_programs": {"type": "array", "items": {"type": "string"}},
                    "categories": {"type": "array", "items": {"type": "string"}}
                }
            },
            "metadata": {
                "type": "object",
                "properties": {
                    "source": {"type": "string"},
                    "extraction_date": {"type": "string"},
                    "content_length": {"type": "integer"}
                }
            }
        },
        "required": ["qa_pairs", "summary"]
    }
    
    # Configure LLM strategy
    llm_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(
            provider="openai/gpt-4o-mini",
            api_token=api_key
        ),
        schema=schema,
        extraction_type="schema",
        instruction=instruction,
        chunk_token_threshold=2000,
        overlap_rate=0.1,
        apply_chunking=True,
        input_format="markdown",
        extra_args={"temperature": 0.1, "max_tokens": 6000}
    )
    
    # Use a dummy crawler just for LLM extraction
    browser_config = BrowserConfig(headless=True, verbose=False)
    
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            # Create a mock result with our content
            from crawl4ai.models import CrawlResult
            
            # Use LLM strategy directly on content
            extracted_data = await llm_strategy.extract(content, {})
            
            if extracted_data:
                # Parse extracted data
                if isinstance(extracted_data, str):
                    extracted_data = json.loads(extracted_data)
                
                # Add metadata
                extracted_data['metadata'] = {
                    'source': 'crawled_ctu_admission_data',
                    'extraction_date': datetime.now().isoformat(),
                    'content_length': len(content),
                    'success': True
                }
                
                # Save to file
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(extracted_data, f, indent=2, ensure_ascii=False)
                
                print(f"‚úÖ Extracted {len(extracted_data.get('qa_pairs', []))} Q&A pairs")
                print(f"üìÑ Saved to: {output_file}")
                
                return extracted_data
            else:
                print("‚ùå No data extracted")
                return None
                
    except Exception as e:
        print(f"‚ùå Exception during extraction: {str(e)}")
        return None

async def main():
    """
    Main function to extract Q&A from crawled content
    """
    # Load environment
    load_dotenv(override=True)
    api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        print("‚ùå Kh√¥ng t√¨m th·∫•y OPENAI_API_KEY!")
        return
    
    # Input and output paths
    crawled_file = "output/crawled_pages/https_tuyensinh.ctu.edu.vn_chuong-trinh-dai-tra_841-danh-muc-nganh-va-chi-tieu-tuyen-sinh-dhcq.html.md"
    output_file = "output/qa_dataset/ctu_admission_qa.json"
    
    # Create output directory
    os.makedirs("output/qa_dataset", exist_ok=True)
    
    print("üöÄ B·∫Øt ƒë·∫ßu extract Q&A t·ª´ n·ªôi dung ƒë√£ crawl...")
    print(f"üìÅ Input: {crawled_file}")
    print(f"üìÅ Output: {output_file}")
    
    # Check if input file exists
    if not Path(crawled_file).exists():
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {crawled_file}")
        return
    
    # Load content
    content = load_crawled_content(crawled_file)
    print(f"üìÑ Loaded content: {len(content)} characters")
    
    # Extract Q&A
    result = await extract_qa_from_content(content, api_key, output_file)
    
    if result:
        print(f"\nüìä K·∫øt qu·∫£ extraction:")
        print(f"   üìù Q&A pairs: {len(result.get('qa_pairs', []))}")
        print(f"   üìö T·ªïng ng√†nh: {result.get('summary', {}).get('total_majors', 'N/A')}")
        print(f"   üë• T·ªïng ch·ªâ ti√™u: {result.get('summary', {}).get('total_quota', 'N/A')}")
        print(f"   üéì Ch∆∞∆°ng tr√¨nh ƒë·∫∑c bi·ªát: {', '.join(result.get('summary', {}).get('special_programs', []))}")
        
        # Show some sample Q&A
        qa_pairs = result.get('qa_pairs', [])
        if qa_pairs:
            print(f"\nüìù M·∫´u Q&A (3 c·∫∑p ƒë·∫ßu):")
            for i, qa in enumerate(qa_pairs[:3], 1):
                print(f"   {i}. Q: {qa['question']}")
                print(f"      A: {qa['answer'][:100]}...")
                print(f"      Category: {qa['category']}, Priority: {qa['priority']}")
                print()

if __name__ == "__main__":
    asyncio.run(main()) 
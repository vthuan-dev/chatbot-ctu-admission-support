import asyncio
import json
import aiohttp
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from datetime import datetime

async def strategy_1_api_direct():
    """
    Chi·∫øn l∆∞·ª£c 1: G·ªçi tr·ª±c ti·∫øp API endpoints v·ªõi POST requests
    D·ª±a tr√™n ph√¢n t√≠ch DevTools, c√°c API n√†y c·∫ßn POST requests
    """
    print("üéØ CHI·∫æN L∆Ø·ª¢C 1: G·ªåI TR·ª∞C TI·∫æP API (POST REQUESTS)")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Referer': 'https://www.ctu.edu.vn/dao-tao/ctdt-dai-hoc.html',
        'Accept': '*/*',
        'Accept-Language': 'vi-VN,vi;q=0.9,fr-FR;q=0.8,fr;q=0.7,en-US;q=0.6,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br, zstd',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'X-Requested-With': 'XMLHttpRequest',
        'Origin': 'https://www.ctu.edu.vn',
        'Connection': 'keep-alive'
    }
    
    # API endpoints v·ªõi POST data d·ª± ƒëo√°n t·ª´ structure
    api_calls = [
        {
            'url': 'https://www.ctu.edu.vn/dao-tao/subject.php',
            'data': {},  # Empty POST ƒë·ªÉ th·ª≠
            'description': 'Get subjects/departments'
        },
        {
            'url': 'https://www.ctu.edu.vn/dao-tao/branch.php', 
            'data': {},  # Empty POST ƒë·ªÉ th·ª≠
            'description': 'Get branches/programs'
        },
        {
            'url': 'https://www.ctu.edu.vn/webctu_program/subject.php',
            'data': {},
            'description': 'Alternative subject endpoint'
        }
    ]
    
    async with aiohttp.ClientSession(headers=headers) as session:
        for api_call in api_calls:
            try:
                print(f"üì° POST Request: {api_call['url']}")
                print(f"üìù Description: {api_call['description']}")
                
                # Th·ª≠ POST request
                async with session.post(api_call['url'], data=api_call['data']) as response:
                    content = await response.text()
                    print(f"  ‚úÖ Status: {response.status}")
                    print(f"  üìÑ Content length: {len(content)}")
                    print(f"  üìã Content preview: {content[:300]}...")
                    
                    # L∆∞u k·∫øt qu·∫£
                    filename = f"output/api_post_{api_call['url'].split('/')[-1]}"
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(content)
                    print(f"  üíæ Saved: {filename}\n")
                    
            except Exception as e:
                print(f"  ‚ùå Error: {e}\n")
                
        # Th·ª≠ th√™m v·ªõi m·ªôt s·ªë parameters c√≥ th·ªÉ
        print("üî¨ Th·ª≠ v·ªõi parameters d·ª± ƒëo√°n...")
        potential_params = [
            {'khoa': '1'},  # Khoa ID
            {'nganh': '1'}, # Ng√†nh ID  
            {'type': 'all'},
            {'action': 'get_data'},
        ]
        
        for params in potential_params:
            try:
                print(f"üì° POST v·ªõi params: {params}")
                async with session.post('https://www.ctu.edu.vn/dao-tao/subject.php', data=params) as response:
                    content = await response.text()
                    if len(content) > 50:  # C√≥ data th·ª±c t·∫ø
                        print(f"  üéØ SUCCESS with params {params}!")
                        print(f"  üìÑ Content length: {len(content)}")
                        filename = f"output/api_success_{str(params).replace(' ', '_')}.html"
                        with open(filename, 'w', encoding='utf-8') as f:
                            f.write(content)
                        print(f"  üíæ Saved: {filename}")
                    else:
                        print(f"  ‚ùå Empty response with params {params}")
            except Exception as e:
                print(f"  ‚ùå Error with {params}: {e}")

async def strategy_2_selenium_simulation():
    """
    Chi·∫øn l∆∞·ª£c 2: Selenium-like interaction ƒë·ªÉ trigger API calls
    """
    print("üéØ CHI·∫æN L∆Ø·ª¢C 2: SIMULATION CLICK EVENTS")
    
    browser_config = BrowserConfig(
        headless=False,  # Ch·∫°y visible ƒë·ªÉ debug
        verbose=True
    )
    
    run_config = CrawlerRunConfig(
        wait_for="css:.program-div-3, table",
        delay_before_return_html=3.0,
        js_code="""
        // H√†m t√¨m v√† click t·∫•t c·∫£ clickable elements
        async function triggerAllClicks() {
            console.log('üîç Finding clickable elements...');
            
            // T√¨m t·∫•t c·∫£ elements c√≥ onclick
            const clickables = document.querySelectorAll('[onclick]');
            console.log('Found onclick elements:', clickables.length);
            
            for (let i = 0; i < clickables.length; i++) {
                const element = clickables[i];
                console.log('Clicking element:', i, element.tagName, element.onclick);
                
                try {
                    element.click();
                    await new Promise(resolve => setTimeout(resolve, 2000)); // Wait 2s
                } catch (e) {
                    console.log('Click error:', e);
                }
            }
            
            // T√¨m c√°c button, link kh√°c
            const buttons = document.querySelectorAll('button, .btn, [role="button"]');
            console.log('Found buttons:', buttons.length);
            
            for (let button of buttons) {
                try {
                    button.click();
                    await new Promise(resolve => setTimeout(resolve, 1500));
                } catch (e) {
                    console.log('Button click error:', e);
                }
            }
            
            // Scroll ƒë·ªÉ trigger lazy loading
            window.scrollTo(0, document.body.scrollHeight);
            await new Promise(resolve => setTimeout(resolve, 3000));
        }
        
        await triggerAllClicks();
        """,
        page_timeout=60000,  # 60 gi√¢y
        simulate_user=True,
        override_navigator=True
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        url = "https://www.ctu.edu.vn/dao-tao/ctdt-dai-hoc.html"
        print(f"üöÄ Crawling with interaction: {url}")
        
        result = await crawler.arun(url=url, config=run_config)
        
        if result.success:
            with open("output/interactive_crawl.md", "w", encoding="utf-8") as f:
                f.write(result.markdown)
            print(f"‚úÖ Interactive crawl saved: {len(result.markdown)} chars")
        else:
            print(f"‚ùå Interactive crawl failed")

async def strategy_3_multiple_endpoints():
    """
    Chi·∫øn l∆∞·ª£c 3: Crawl nhi·ªÅu endpoints kh√°c nhau c·ªßa CTU
    """
    print("üéØ CHI·∫æN L∆Ø·ª¢C 3: CRAWL MULTIPLE CTU ENDPOINTS")
    
    # Danh s√°ch URLs c√≥ th·ªÉ c√≥ th√¥ng tin ng√†nh h·ªçc
    ctu_urls = [
        "https://tuyensinh.ctu.edu.vn/",  # Trang tuy·ªÉn sinh ch√≠nh
        "https://tuyensinh.ctu.edu.vn/nganh-va-chi-tieu",
        "https://tuyensinh.ctu.edu.vn/chuong-trinh-dao-tao",
        "https://www.ctu.edu.vn/dao-tao",
        "https://www.ctu.edu.vn/gioi-thieu/don-vi",
        "https://ctc.ctu.edu.vn/",  # Ch∆∞∆°ng tr√¨nh ch·∫•t l∆∞·ª£ng cao
        "https://gs.ctu.edu.vn/"   # Sau ƒë·∫°i h·ªçc
    ]
    
    browser_config = BrowserConfig(headless=True, verbose=True)
    run_config = CrawlerRunConfig(
        delay_before_return_html=3.0,
        simulate_user=True
    )
    
    all_content = []
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        for i, url in enumerate(ctu_urls, 1):
            try:
                print(f"üì° [{i}/{len(ctu_urls)}] Crawling: {url}")
                result = await crawler.arun(url=url, config=run_config)
                
                if result.success and result.markdown:
                    content_info = {
                        'url': url,
                        'content': result.markdown,
                        'length': len(result.markdown),
                        'timestamp': datetime.now().isoformat()
                    }
                    all_content.append(content_info)
                    print(f"  ‚úÖ Success: {len(result.markdown)} chars")
                else:
                    print(f"  ‚ùå Failed: {url}")
                    
            except Exception as e:
                print(f"  ‚ùå Error: {e}")
    
    # Combine all content
    if all_content:
        combined_markdown = f"# CTU COMBINED CRAWL RESULTS - {datetime.now()}\n\n"
        
        for content_info in all_content:
            combined_markdown += f"## Source: {content_info['url']}\n"
            combined_markdown += f"Length: {content_info['length']} characters\n"
            combined_markdown += f"Crawled: {content_info['timestamp']}\n\n"
            combined_markdown += content_info['content']
            combined_markdown += "\n\n---\n\n"
        
        with open("output/ctu_combined_crawl.md", "w", encoding="utf-8") as f:
            f.write(combined_markdown)
        
        print(f"‚úÖ Combined crawl saved: {len(combined_markdown)} chars from {len(all_content)} sources")
        return combined_markdown
    
    return None

async def main():
    """
    Ch·∫°y t·∫•t c·∫£ chi·∫øn l∆∞·ª£c crawling
    """
    import os
    os.makedirs("output", exist_ok=True)
    
    print("üöÄ B·∫ÆT ƒê·∫¶U CRAWL CTU V·ªöI MULTIPLE STRATEGIES\n")
    
    # Chi·∫øn l∆∞·ª£c 1: API Direct
    await strategy_1_api_direct()
    
    print("\n" + "="*50 + "\n")
    
    # Chi·∫øn l∆∞·ª£c 2: Interactive Simulation  
    await strategy_2_selenium_simulation()
    
    print("\n" + "="*50 + "\n")
    
    # Chi·∫øn l∆∞·ª£c 3: Multiple Endpoints (RECOMMENDED)
    combined_content = await strategy_3_multiple_endpoints()
    
    if combined_content:
        print(f"\nüéä TH√ÄNH C√îNG! ƒê√£ thu th·∫≠p ƒë∆∞·ª£c {len(combined_content)} k√Ω t·ª±")
        print("üìù Khuy·∫øn ngh·ªã: S·ª≠ d·ª•ng file 'ctu_combined_crawl.md' ƒë·ªÉ extract Q&A")
        print("üîß Ch·∫°y ti·∫øp: python 2.llm_extract.py")

if __name__ == "__main__":
    asyncio.run(main()) 
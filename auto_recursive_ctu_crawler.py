import asyncio
import json
import os
from pathlib import Path
from datetime import datetime
from typing import List, Set, Dict
import time

from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from dotenv import load_dotenv

class AutoRecursiveCTUCrawler:
    """
    ğŸ¤– Fully Automated Recursive CTU Crawler
    
    Workflow:
    1. Start with initial URL
    2. Crawl â†’ Convert to .md â†’ Extract to JSON
    3. Find new URLs in JSON
    4. Crawl new URLs â†’ Extract â†’ Find more URLs
    5. Continue until no new URLs or max depth reached
    6. Organize by intent automatically
    7. You can rest! ğŸ˜´
    """
    
    def __init__(self, api_key: str, max_depth: int = 5, max_urls_per_level: int = 10):
        self.api_key = api_key
        self.max_depth = max_depth
        self.max_urls_per_level = max_urls_per_level
        
        # Tracking
        self.crawled_urls: Set[str] = set()
        self.all_qa_pairs: List[Dict] = []
        self.intent_data: Dict[str, List] = {
            # Core admission intents
            'hoi_nganh_hoc': [],                    # NgÃ nh há»c, chuyÃªn ngÃ nh
            'hoi_phuong_thuc_xet_tuyen': [],       # PhÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn
            'hoi_hoc_phi': [],                     # Há»c phÃ­, chi phÃ­
            'hoi_lien_he': [],                     # LiÃªn há»‡, Ä‘á»‹a chá»‰
            
            # Extended admission intents
            'hoi_diem_chuan': [],                  # Äiá»ƒm chuáº©n, Ä‘iá»ƒm xÃ©t tuyá»ƒn
            'hoi_ho_so_xet_tuyen': [],            # Há»“ sÆ¡ xÃ©t tuyá»ƒn, thá»§ tá»¥c
            'hoi_lich_tuyen_sinh': [],            # Lá»‹ch tuyá»ƒn sinh, thá»i gian
            'hoi_hoc_bong': [],                   # Há»c bá»•ng, há»— trá»£ tÃ i chÃ­nh
            'hoi_co_so_vat_chat': [],             # CÆ¡ sá»Ÿ váº­t cháº¥t, kÃ½ tÃºc xÃ¡
            'hoi_sinh_vien_quoc_te': [],          # Sinh viÃªn quá»‘c táº¿
            'hoi_chuong_trinh_lien_ket': [],      # ChÆ°Æ¡ng trÃ¬nh liÃªn káº¿t
            'hoi_thuc_tap_viec_lam': [],          # Thá»±c táº­p, viá»‡c lÃ m sau tá»‘t nghiá»‡p
            'hoi_hoat_dong_sinh_vien': [],        # Hoáº¡t Ä‘á»™ng sinh viÃªn, CLB
            'hoi_dao_tao_sau_dai_hoc': [],        # ÄÃ o táº¡o sau Ä‘áº¡i há»c
            'hoi_thong_tin_chung': []             # ThÃ´ng tin chung khÃ¡c
        }
        
        # Setup directories
        self.setup_directories()
        
    def setup_directories(self):
        """Setup output directories"""
        directories = [
            'output/auto_recursive',
            'output/auto_recursive/markdown',
            'output/auto_recursive/json',
            'output/auto_recursive/by_intent',
            'data/auto_recursive'
        ]
        for dir_path in directories:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    def is_valid_ctu_url(self, url: str) -> bool:
        """Check if URL is valid CTU admission related"""
        if not url or not isinstance(url, str):
            return False
            
        # Must contain CTU indicators
        ctu_indicators = ['ctu.edu.vn', 'tuyensinh', 'nganh', 'hoc-phi', 'xet-tuyen']
        if not any(indicator in url.lower() for indicator in ctu_indicators):
            return False
            
        # Skip unwanted file types
        skip_extensions = ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar']
        if any(url.lower().endswith(ext) for ext in skip_extensions):
            return False
            
        # Skip already crawled
        if url in self.crawled_urls:
            return False
            
        return True
    
    def detect_intent(self, question: str, answer: str) -> str:
        """Smart intent detection based on keywords"""
        text = (question + " " + answer).lower()
        
        # Enhanced intent keyword mapping
        intent_keywords = {
            # Core admission intents
            'hoi_nganh_hoc': ['ngÃ nh', 'chuyÃªn ngÃ nh', 'Ä‘Ã o táº¡o', 'chÆ°Æ¡ng trÃ¬nh', 'major', 'faculty', 'khoa', 'bá»™ mÃ´n'],
            'hoi_phuong_thuc_xet_tuyen': ['xÃ©t tuyá»ƒn', 'phÆ°Æ¡ng thá»©c', 'thi', 'admission', 'entrance', 'tuyá»ƒn sinh'],
            'hoi_hoc_phi': ['há»c phÃ­', 'chi phÃ­', 'tuition', 'fee', 'cost', 'tiá»n há»c', 'lá»‡ phÃ­'],
            'hoi_lien_he': ['liÃªn há»‡', 'contact', 'phone', 'email', 'address', 'Ä‘á»‹a chá»‰', 'hotline', 'tÆ° váº¥n'],
            
            # Extended admission intents
            'hoi_diem_chuan': ['Ä‘iá»ƒm chuáº©n', 'Ä‘iá»ƒm xÃ©t tuyá»ƒn', 'Ä‘iá»ƒm thi', 'Ä‘iá»ƒm sá»‘', 'cut-off', 'benchmark'],
            'hoi_ho_so_xet_tuyen': ['há»“ sÆ¡', 'thá»§ tá»¥c', 'giáº¥y tá»', 'Ä‘Äƒng kÃ½', 'ná»™p há»“ sÆ¡', 'documents', 'application'],
            'hoi_lich_tuyen_sinh': ['lá»‹ch', 'thá»i gian', 'deadline', 'háº¡n chÃ³t', 'schedule', 'calendar', 'ngÃ y'],
            'hoi_hoc_bong': ['há»c bá»•ng', 'há»— trá»£', 'scholarship', 'tÃ i chÃ­nh', 'miá»…n giáº£m', 'trá»£ cáº¥p'],
            'hoi_co_so_vat_chat': ['cÆ¡ sá»Ÿ váº­t cháº¥t', 'kÃ½ tÃºc xÃ¡', 'dormitory', 'facilities', 'infrastructure', 'ktx'],
            'hoi_sinh_vien_quoc_te': ['quá»‘c táº¿', 'international', 'nÆ°á»›c ngoÃ i', 'foreign', 'exchange'],
            'hoi_chuong_trinh_lien_ket': ['liÃªn káº¿t', 'partnership', 'collaboration', 'joint program', 'há»£p tÃ¡c'],
            'hoi_thuc_tap_viec_lam': ['thá»±c táº­p', 'viá»‡c lÃ m', 'internship', 'job', 'career', 'employment', 'tá»‘t nghiá»‡p'],
            'hoi_hoat_dong_sinh_vien': ['hoáº¡t Ä‘á»™ng', 'sinh viÃªn', 'clb', 'club', 'activities', 'extracurricular'],
            'hoi_dao_tao_sau_dai_hoc': ['sau Ä‘áº¡i há»c', 'tháº¡c sÄ©', 'tiáº¿n sÄ©', 'master', 'phd', 'graduate', 'postgraduate'],
            'hoi_thong_tin_chung': ['thÃ´ng tin', 'general', 'about', 'overview', 'giá»›i thiá»‡u', 'tá»•ng quan']
        }
        
        for intent, keywords in intent_keywords.items():
            if any(keyword in text for keyword in keywords):
                return intent
                
        return 'hoi_thong_tin_chung'  # Default
    
    async def crawl_and_extract_single_url(self, url: str, level: int) -> Dict:
        """Crawl single URL and extract to JSON"""
        print(f"ğŸ•·ï¸  Level {level}: Crawling {url}")
        
        # Browser config
        browser_config = BrowserConfig(headless=True, verbose=False)
        
        # LLM extraction strategy
        instruction = f"""
        PhÃ¢n tÃ­ch ná»™i dung tuyá»ƒn sinh Äáº¡i há»c Cáº§n ThÆ¡ vÃ  trÃ­ch xuáº¥t:
        
        1. Táº¡o 3-7 cáº·p cÃ¢u há»i-tráº£ lá»i tiáº¿ng Viá»‡t vá» tuyá»ƒn sinh
        2. TÃ¬m táº¥t cáº£ URLs liÃªn quan Ä‘áº¿n tuyá»ƒn sinh CTU
        
        YÃªu cáº§u:
        - CÃ¢u há»i vÃ  tráº£ lá»i pháº£i báº±ng tiáº¿ng Viá»‡t
        - ThÃ´ng tin chÃ­nh xÃ¡c vÃ  chi tiáº¿t
        - URLs pháº£i liÃªn quan Ä‘áº¿n tuyá»ƒn sinh CTU
        
        Tráº£ vá» JSON há»£p lá»‡.
        """
        
        schema = {
            "type": "object",
            "properties": {
                "qa_pairs": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "question": {"type": "string"},
                            "answer": {"type": "string"},
                            "category": {"type": "string"},
                            "priority": {"type": "integer", "minimum": 1, "maximum": 3}
                        },
                        "required": ["question", "answer", "category", "priority"]
                    }
                },
                "urls": {
                    "type": "array",
                    "items": {"type": "string"}
                }
            },
            "required": ["qa_pairs", "urls"]
        }
        
        llm_strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(
                provider="openai/gpt-4o-mini",
                api_token=self.api_key
            ),
            schema=schema,
            extraction_type="schema",
            instruction=instruction,
            chunk_token_threshold=1200,
            overlap_rate=0.1
        )
        
        run_config = CrawlerRunConfig(
            extraction_strategy=llm_strategy,
            cache_mode=CacheMode.BYPASS
        )
        
        try:
            async with AsyncWebCrawler(config=browser_config) as crawler:
                result = await crawler.arun(url=url, config=run_config)
                
                if not result.success:
                    print(f"âŒ Failed to crawl {url}: {result.error_message}")
                    return {"qa_pairs": [], "urls": [], "success": False}
                
                # Save markdown
                url_filename = url.replace("://", "_").replace("/", "_").replace("?", "_")[:100]
                md_file = f"output/auto_recursive/markdown/level_{level}_{url_filename}.md"
                with open(md_file, "w", encoding="utf-8") as f:
                    f.write(result.markdown.raw_markdown if result.markdown else "")
                
                # Process extracted content
                if result.extracted_content:
                    try:
                        extracted_data = json.loads(result.extracted_content) if isinstance(result.extracted_content, str) else result.extracted_content
                        
                        # Handle case where OpenAI returns a list instead of dict
                        if isinstance(extracted_data, list):
                            if len(extracted_data) > 0 and isinstance(extracted_data[0], dict):
                                extracted_data = extracted_data[0]  # Take first item
                            else:
                                extracted_data = {"qa_pairs": [], "urls": []}
                        
                        # Ensure required keys exist
                        if not isinstance(extracted_data, dict):
                            extracted_data = {"qa_pairs": [], "urls": []}
                        
                        qa_pairs = extracted_data.get('qa_pairs', [])
                        urls = extracted_data.get('urls', [])
                        
                        # Save JSON
                        json_file = f"output/auto_recursive/json/level_{level}_{url_filename}.json"
                        with open(json_file, "w", encoding="utf-8") as f:
                            json.dump(extracted_data, f, indent=2, ensure_ascii=False)
                        
                        print(f"âœ… Extracted {len(qa_pairs)} Q&A pairs and {len(urls)} URLs")
                        
                        return {
                            "qa_pairs": qa_pairs,
                            "urls": urls,
                            "success": True,
                            "source_url": url,
                            "level": level,
                            "crawl_time": datetime.now().isoformat()
                        }
                        
                    except Exception as e:
                        print(f"âŒ Error parsing extracted content from {url}: {e}")
                        return {"qa_pairs": [], "urls": [], "success": False}
                else:
                    print(f"âš ï¸  No extracted content from {url}")
                    return {"qa_pairs": [], "urls": [], "success": False}
                    
        except Exception as e:
            print(f"âŒ Error crawling {url}: {e}")
            return {"qa_pairs": [], "urls": [], "success": False}
    
    def organize_qa_by_intent(self, qa_pairs: List[Dict], source_url: str, level: int):
        """Organize Q&A pairs by intent"""
        for qa in qa_pairs:
            intent = self.detect_intent(qa['question'], qa['answer'])
            
            # Add metadata
            qa_with_metadata = {
                **qa,
                'intent': intent,
                'source_url': source_url,
                'level': level,
                'id': f"qa_{len(self.all_qa_pairs) + 1:04d}",
                'created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            self.intent_data[intent].append(qa_with_metadata)
            self.all_qa_pairs.append(qa_with_metadata)
    
    def save_intent_data(self):
        """Save data organized by intent"""
        for intent, qa_list in self.intent_data.items():
            if qa_list:
                intent_file = f"output/auto_recursive/by_intent/{intent}.json"
                intent_dataset = {
                    'intent': intent,
                    'count': len(qa_list),
                    'updated_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'qa_pairs': qa_list
                }
                
                with open(intent_file, 'w', encoding='utf-8') as f:
                    json.dump(intent_dataset, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ’¾ Saved {len(qa_list)} Q&A pairs for intent: {intent}")
    
    def save_final_dataset(self):
        """Save final comprehensive dataset"""
        total_pairs = sum(len(pairs) for pairs in self.intent_data.values())
        
        final_dataset = {
            'dataset_info': {
                'name': 'CTU Auto Recursive Crawl Dataset',
                'version': '1.0',
                'description': 'Fully automated recursive crawling of CTU admission website',
                'total_pairs': total_pairs,
                'total_intents': len([intent for intent, pairs in self.intent_data.items() if pairs]),
                'created_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'crawled_urls_count': len(self.crawled_urls),
                'max_depth_reached': self.max_depth,
                'source': 'Auto Recursive Crawler'
            },
            'intents': {intent: len(pairs) for intent, pairs in self.intent_data.items()},
            'qa_pairs': self.all_qa_pairs,
            'crawled_urls': list(self.crawled_urls)
        }
        
        final_file = 'data/auto_recursive/ctu_auto_recursive_dataset.json'
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(final_dataset, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ‰ Final dataset saved: {final_file}")
        print(f"ğŸ“Š Total Q&A pairs: {total_pairs}")
        print(f"ğŸ•·ï¸ Total URLs crawled: {len(self.crawled_urls)}")
    
    async def run_recursive_crawl(self, start_url: str):
        """ğŸš€ Main recursive crawling workflow"""
        print("ğŸ¤– Starting Auto Recursive CTU Crawler...")
        print(f"ğŸ¯ Start URL: {start_url}")
        print(f"ğŸ“ Max depth: {self.max_depth}")
        print(f"ğŸ”¢ Max URLs per level: {self.max_urls_per_level}")
        print("ğŸ˜´ You can rest now! The system will work automatically...\n")
        
        current_urls = [start_url]
        level = 1
        
        while level <= self.max_depth and current_urls:
            print(f"\nğŸ”„ === LEVEL {level} === ({len(current_urls)} URLs to crawl)")
            
            next_level_urls = []
            
            # Process each URL in current level
            for i, url in enumerate(current_urls[:self.max_urls_per_level], 1):
                if not self.is_valid_ctu_url(url):
                    continue
                
                print(f"[{i}/{min(len(current_urls), self.max_urls_per_level)}] ", end="")
                
                # Crawl and extract
                result = await self.crawl_and_extract_single_url(url, level)
                
                if result['success']:
                    # Mark as crawled
                    self.crawled_urls.add(url)
                    
                    # Organize Q&A by intent
                    qa_pairs = result.get('qa_pairs', [])
                    if qa_pairs:
                        self.organize_qa_by_intent(qa_pairs, url, level)
                    
                    # Collect new URLs for next level
                    new_urls = result.get('urls', [])
                    for new_url in new_urls:
                        if self.is_valid_ctu_url(new_url):
                            next_level_urls.append(new_url)
                
                # Be polite - small delay
                await asyncio.sleep(1)
            
            # Remove duplicates for next level
            current_urls = list(set(next_level_urls))
            level += 1
            
            # Save progress after each level
            self.save_intent_data()
            
            print(f"\nğŸ“ˆ Level {level-1} completed!")
            print(f"   Q&A pairs collected: {len(self.all_qa_pairs)}")
            print(f"   URLs found for next level: {len(current_urls)}")
        
        # Save final results
        print(f"\nğŸ‰ Auto Recursive Crawling COMPLETED!")
        print(f"ğŸ Reached level {level-1} (max: {self.max_depth})")
        self.save_final_dataset()
        
        # Summary
        print(f"\nğŸ“Š === FINAL SUMMARY ===")
        for intent, pairs in self.intent_data.items():
            if pairs:
                print(f"   {intent}: {len(pairs)} Q&A pairs")
        
        print(f"\nğŸ’¤ You can wake up now! Everything is done automatically! ğŸ‰")

async def main():
    """Main function to run the auto recursive crawler"""
    load_dotenv(override=True)
    
    # Configuration
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ Error: No OpenAI API key found. Set OPENAI_API_KEY environment variable.")
        return
    
    start_url = "https://tuyensinh.ctu.edu.vn/"
    max_depth = 4  # Adjust as needed
    max_urls_per_level = 8  # Adjust as needed
    
    # Create and run crawler
    crawler = AutoRecursiveCTUCrawler(
        api_key=api_key,
        max_depth=max_depth,
        max_urls_per_level=max_urls_per_level
    )
    
    await crawler.run_recursive_crawl(start_url)

if __name__ == "__main__":
    asyncio.run(main()) 
import asyncio
import json
import os
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
import openai


class CrawledFilesExtractor:
    def __init__(self, api_key: str):
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.intents = {
            "nganh_hoc": {
                "folder": "data/processed/nganh_hoc",
                "description": "ThÃ´ng tin vá» cÃ¡c ngÃ nh há»c, chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o",
                "keywords": ["ngÃ nh", "chuyÃªn ngÃ nh", "Ä‘Ã o táº¡o", "chÆ°Æ¡ng trÃ¬nh", "khoa", "báº±ng cáº¥p", "tuyá»ƒn sinh", "mÃ£ ngÃ nh", "chá»‰ tiÃªu"]
            },
            "xet_tuyen": {
                "folder": "data/processed/xet_tuyen", 
                "description": "PhÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn, Ä‘iá»u kiá»‡n, thá»§ tá»¥c",
                "keywords": ["xÃ©t tuyá»ƒn", "phÆ°Æ¡ng thá»©c", "Ä‘iá»u kiá»‡n", "thá»§ tá»¥c", "há»“ sÆ¡", "Ä‘Äƒng kÃ½", "tá»• há»£p", "Ä‘iá»ƒm chuáº©n", "tuyá»ƒn tháº³ng"]
            },
            "hoc_phi": {
                "folder": "data/processed/hoc_phi",
                "description": "Há»c phÃ­, chi phÃ­, há»c bá»•ng",
                "keywords": ["há»c phÃ­", "chi phÃ­", "há»c bá»•ng", "miá»…n giáº£m", "tÃ i chÃ­nh", "kinh phÃ­", "phÃ­", "tiá»n", "há»c bá»•ng"]
            },
            "lien_he": {
                "folder": "data/processed/lien_he",
                "description": "ThÃ´ng tin liÃªn há»‡, Ä‘á»‹a chá»‰, Ä‘iá»‡n thoáº¡i",
                "keywords": ["liÃªn há»‡", "Ä‘á»‹a chá»‰", "Ä‘iá»‡n thoáº¡i", "email", "tÆ° váº¥n", "hotline", "contact", "phone", "address"]
            },
            "sinh_vien": {
                "folder": "data/processed/sinh_vien",
                "description": "Hoáº¡t Ä‘á»™ng sinh viÃªn, cÃ¢u láº¡c bá»™, Ä‘oÃ n thá»ƒ",
                "keywords": ["sinh viÃªn", "hoáº¡t Ä‘á»™ng", "cÃ¢u láº¡c bá»™", "Ä‘oÃ n", "há»™i", "sá»± kiá»‡n", "festival", "thi Ä‘áº¥u", "thá»ƒ thao", "vÄƒn nghá»‡"]
            },
            "nghien_cuu": {
                "folder": "data/processed/nghien_cuu",
                "description": "NghiÃªn cá»©u khoa há»c, dá»± Ã¡n, cÃ´ng bá»‘",
                "keywords": ["nghiÃªn cá»©u", "khoa há»c", "dá»± Ã¡n", "cÃ´ng bá»‘", "bÃ i bÃ¡o", "há»™i tháº£o", "seminar", "research", "publication"]
            },
            "sau_dai_hoc": {
                "folder": "data/processed/sau_dai_hoc",
                "description": "ChÆ°Æ¡ng trÃ¬nh sau Ä‘áº¡i há»c, tháº¡c sÄ©, tiáº¿n sÄ©",
                "keywords": ["sau Ä‘áº¡i há»c", "tháº¡c sÄ©", "tiáº¿n sÄ©", "cao há»c", "graduate", "master", "phd", "doctorate", "postgraduate"]
            },
            "quoc_te": {
                "folder": "data/processed/quoc_te",
                "description": "Há»£p tÃ¡c quá»‘c táº¿, trao Ä‘á»•i sinh viÃªn, chÆ°Æ¡ng trÃ¬nh liÃªn káº¿t",
                "keywords": ["quá»‘c táº¿", "há»£p tÃ¡c", "trao Ä‘á»•i", "liÃªn káº¿t", "international", "exchange", "partnership", "abroad", "global"]
            },
            "dich_vu": {
                "folder": "data/processed/dich_vu",
                "description": "Dá»‹ch vá»¥ sinh viÃªn, thÆ° viá»‡n, kÃ½ tÃºc xÃ¡, y táº¿",
                "keywords": ["dá»‹ch vá»¥", "thÆ° viá»‡n", "kÃ½ tÃºc xÃ¡", "y táº¿", "canteen", "library", "dormitory", "medical", "service", "facility"]
            },
            "cuu_sinh_vien": {
                "folder": "data/processed/cuu_sinh_vien",
                "description": "Cá»±u sinh viÃªn, máº¡ng lÆ°á»›i alumni, viá»‡c lÃ m",
                "keywords": ["cá»±u sinh viÃªn", "alumni", "viá»‡c lÃ m", "career", "job", "employment", "máº¡ng lÆ°á»›i", "network", "graduate"]
            },
            "xuat_ban": {
                "folder": "data/processed/xuat_ban",
                "description": "Xuáº¥t báº£n, táº¡p chÃ­, sÃ¡ch, tÃ i liá»‡u",
                "keywords": ["xuáº¥t báº£n", "táº¡p chÃ­", "sÃ¡ch", "tÃ i liá»‡u", "publication", "journal", "book", "document", "material"]
            },
            "thong_tin": {
                "folder": "data/processed/thong_tin",
                "description": "ThÃ´ng tin chung vá» trÆ°á»ng, cÆ¡ sá»Ÿ váº­t cháº¥t, lá»‹ch sá»­",
                "keywords": ["giá»›i thiá»‡u", "lá»‹ch sá»­", "cÆ¡ sá»Ÿ", "thÃ´ng tin chung", "táº§m nhÃ¬n", "sá»© má»‡nh", "hoáº¡t Ä‘á»™ng", "campus", "history", "about"]
            }
        }
    
    def classify_content_by_intent(self, content: str, filename: str) -> str:
        """PhÃ¢n loáº¡i ná»™i dung theo intent dá»±a trÃªn keywords vÃ  filename"""
        # PhÃ¢n loáº¡i dá»±a trÃªn tÃªn file trÆ°á»›c
        filename_lower = filename.lower()
        
        # PhÃ¢n loáº¡i theo tÃªn file cá»¥ thá»ƒ
        if any(keyword in filename_lower for keyword in ["cet", "cit", "cse", "coa", "se", "caf", "cns", "sps", "sl", "sfl", "nganh", "major", "program"]):
            return "nganh_hoc"
        elif any(keyword in filename_lower for keyword in ["daa", "tuyensinh", "admission", "xet-tuyen", "phuong-thuc"]):
            return "xet_tuyen"
        elif any(keyword in filename_lower for keyword in ["dfa", "tai_chinh", "hoc-phi", "tuition", "fee", "scholarship"]):
            return "hoc_phi"
        elif any(keyword in filename_lower for keyword in ["dsa", "lien_he", "contact", "phone", "address"]):
            return "lien_he"
        elif any(keyword in filename_lower for keyword in ["student", "sinh-vien", "club", "activity", "event", "festival"]):
            return "sinh_vien"
        elif any(keyword in filename_lower for keyword in ["research", "nghien-cuu", "khoa-hoc", "publication", "seminar"]):
            return "nghien_cuu"
        elif any(keyword in filename_lower for keyword in ["graduate", "sau-dai-hoc", "thac-si", "tien-si", "master", "phd"]):
            return "sau_dai_hoc"
        elif any(keyword in filename_lower for keyword in ["international", "quoc-te", "cooperation", "exchange", "abroad"]):
            return "quoc_te"
        elif any(keyword in filename_lower for keyword in ["service", "dich-vu", "library", "dormitory", "facility", "medical"]):
            return "dich_vu"
        elif any(keyword in filename_lower for keyword in ["alumni", "cuu-sinh-vien", "career", "job", "employment"]):
            return "cuu_sinh_vien"
        elif any(keyword in filename_lower for keyword in ["publication", "xuat-ban", "journal", "book", "document"]):
            return "xuat_ban"
        
        # Náº¿u khÃ´ng phÃ¢n loáº¡i Ä‘Æ°á»£c tá»« filename, dÃ¹ng content analysis
        content_lower = content.lower()
        best_intent = "thong_tin"  # Default intent
        max_score = 0
        
        for intent_name, intent_info in self.intents.items():
            score = 0
            for keyword in intent_info["keywords"]:
                # TÄƒng trá»ng sá»‘ cho keywords xuáº¥t hiá»‡n nhiá»u láº§n
                keyword_count = content_lower.count(keyword.lower())
                score += keyword_count * (2 if len(keyword) > 5 else 1)  # Tá»« dÃ i hÆ¡n cÃ³ trá»ng sá»‘ cao hÆ¡n
            
            # Bonus Ä‘iá»ƒm náº¿u cÃ³ nhiá»u keywords cá»§a cÃ¹ng intent
            keyword_matches = sum(1 for keyword in intent_info["keywords"] if keyword.lower() in content_lower)
            if keyword_matches >= 3:
                score += keyword_matches * 5
            
            if score > max_score:
                max_score = score
                best_intent = intent_name
        
        return best_intent
    
    async def extract_qa_pairs_from_content(self, content: str, intent: str, filename: str) -> List[Dict[str, Any]]:
        """TrÃ­ch xuáº¥t Q&A pairs tá»« ná»™i dung markdown"""
        if not content.strip() or len(content) < 200:
            return []
        
        intent_info = self.intents[intent]
        
        # Giá»›i háº¡n Ä‘á»™ dÃ i content Ä‘á»ƒ trÃ¡nh vÆ°á»£t quÃ¡ token limit
        content_chunk = content[:6000] if len(content) > 6000 else content
        
        prompt = f"""
        Báº¡n lÃ  chuyÃªn gia trÃ­ch xuáº¥t dá»¯ liá»‡u cho chatbot tÆ° váº¥n tuyá»ƒn sinh Äáº¡i há»c Cáº§n ThÆ¡.
        
        Tá»« ná»™i dung markdown sau vá» "{intent_info['description']}" (tá»« file: {filename}), hÃ£y táº¡o cÃ¡c cáº·p cÃ¢u há»i-tráº£ lá»i (Q&A) báº±ng tiáº¿ng Viá»‡t:
        
        Ná»˜I DUNG:
        {content_chunk}
        
        YÃŠU Cáº¦U QUAN TRá»ŒNG:
        1. Táº¡o 5-15 cáº·p cÃ¢u há»i-tráº£ lá»i tá»± nhiÃªn
        2. CÃ¢u há»i pháº£i nhÆ° sinh viÃªn tháº­t sáº½ há»i (cá»¥ thá»ƒ, thá»±c táº¿)
        3. âš ï¸ CÃ‚U TRáº¢ Lá»œI PHáº¢I:
           - Tráº£ lá»i TRá»°C TIáº¾P cÃ¢u há»i, khÃ´ng nÃ³i "báº¡n cÃ³ thá»ƒ tÃ¬m hiá»ƒu thÃªm"
           - Cung cáº¥p thÃ´ng tin Cá»¤ THá»‚ tá»« ná»™i dung (sá»‘ liá»‡u, mÃ£ ngÃ nh, tÃªn cá»¥ thá»ƒ)
           - Náº¿u khÃ´ng cÃ³ thÃ´ng tin trong ná»™i dung thÃ¬ KHÃ”NG táº¡o cÃ¢u há»i Ä‘Ã³
           - TrÃ¡nh cÃ¢u tráº£ lá»i chung chung nhÆ° "cÃ³ nhiá»u ngÃ nh" â†’ pháº£i liá»‡t kÃª cá»¥ thá»ƒ
           - TrÃ¡nh hÆ°á»›ng dáº«n "liÃªn há»‡ Ä‘á»ƒ biáº¿t thÃªm" â†’ tráº£ lá»i tháº³ng thÃ´ng tin cÃ³ sáºµn
        4. Táº­p trung vÃ o chá»§ Ä‘á»: {intent_info['description']}
        5. Æ¯u tiÃªn thÃ´ng tin Ä‘á»‹nh lÆ°á»£ng: mÃ£ ngÃ nh, chá»‰ tiÃªu, há»c phÃ­, Ä‘iá»ƒm chuáº©n, thá»i gian
        6. Má»—i cÃ¢u tráº£ lá»i pháº£i cÃ³ Ã­t nháº¥t 1 thÃ´ng tin cá»¥ thá»ƒ (sá»‘, tÃªn, Ä‘á»‹a chá»‰, ngÃ y thÃ¡ng)
        
        VÃ Dá»¤ CÃCH TRáº¢ Lá»œI Tá»T:
        âŒ SAI: "TrÆ°á»ng cÃ³ nhiá»u ngÃ nh há»c, báº¡n cÃ³ thá»ƒ tham kháº£o trÃªn website"
        âœ… ÄÃšNG: "TrÆ°á»ng cÃ³ cÃ¡c ngÃ nh: CÃ´ng nghá»‡ thÃ´ng tin (mÃ£ 7480201, 300 chá»‰ tiÃªu), Kinh táº¿ (mÃ£ 7310101, 200 chá»‰ tiÃªu), Quáº£n trá»‹ kinh doanh (mÃ£ 7340101, 250 chá»‰ tiÃªu)"
        
        âŒ SAI: "Há»c phÃ­ thay Ä‘á»•i theo tá»«ng ngÃ nh, liÃªn há»‡ Ä‘á»ƒ biáº¿t chi tiáº¿t"
        âœ… ÄÃšNG: "Há»c phÃ­ nÄƒm 2025: NgÃ nh CNTT 15 triá»‡u/nÄƒm, Kinh táº¿ 12 triá»‡u/nÄƒm, Y khoa 25 triá»‡u/nÄƒm"
        
        âŒ SAI: "CÃ³ nhiá»u phÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn"
        âœ… ÄÃšNG: "CÃ³ 4 phÆ°Æ¡ng thá»©c xÃ©t tuyá»ƒn: Äiá»ƒm thi THPT (70%), Há»c báº¡ (20%), NÄƒng khiáº¿u (5%), Æ¯u tiÃªn khu vá»±c (5%)"
        
        Tráº£ vá» JSON format:
        {{
            "intent": "{intent}",
            "source_file": "{filename}",
            "qa_pairs": [
                {{
                    "question": "CÃ¢u há»i cá»¥ thá»ƒ cá»§a sinh viÃªn?",
                    "answer": "CÃ¢u tráº£ lá»i CHI TIáº¾T, Cá»¤ THá»‚ vá»›i sá»‘ liá»‡u/tÃªn/thÃ´ng tin Ä‘á»‹nh lÆ°á»£ng tá»« ná»™i dung",
                    "category": "{intent}",
                    "confidence": 0.9,
                    "source": "{filename}"
                }}
            ]
        }}
        """
        
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Báº¡n lÃ  chuyÃªn gia trÃ­ch xuáº¥t dá»¯ liá»‡u cho chatbot tÆ° váº¥n tuyá»ƒn sinh. LuÃ´n tráº£ vá» JSON há»£p lá»‡."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=3000
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # LÃ m sáº¡ch JSON response
            if result_text.startswith("```json"):
                result_text = result_text[7:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
            
            result = json.loads(result_text)
            return result.get("qa_pairs", [])
            
        except Exception as e:
            print(f"Error extracting Q&A from {filename}: {e}")
            return []
    
    async def process_crawled_files(self, crawled_dir: str = "output/crawled_ctu_admission_pages"):
        """Xá»­ lÃ½ táº¥t cáº£ file markdown Ä‘Ã£ cÃ o"""
        crawled_path = Path(crawled_dir)
        
        if not crawled_path.exists():
            print(f"âŒ Directory not found: {crawled_dir}")
            return
        
        # Láº¥y táº¥t cáº£ file .md
        md_files = list(crawled_path.glob("*.md"))
        
        if not md_files:
            print(f"âŒ No markdown files found in {crawled_dir}")
            return
        
        print(f"ğŸ” Found {len(md_files)} markdown files to process")
        
        # Táº¡o thÆ° má»¥c output
        for intent_name, intent_info in self.intents.items():
            folder_path = Path(intent_info["folder"])
            folder_path.mkdir(parents=True, exist_ok=True)
        
        # Xá»­ lÃ½ tá»«ng file
        total_qa_pairs = 0
        intent_counts = {intent: 0 for intent in self.intents.keys()}
        
        for i, md_file in enumerate(md_files, 1):
            print(f"\nğŸ“„ Processing {i}/{len(md_files)}: {md_file.name}")
            
            try:
                # Äá»c ná»™i dung file
                with open(md_file, "r", encoding="utf-8") as f:
                    content = f.read()
                
                if len(content) < 200:
                    print(f"   âš ï¸ File too short, skipping")
                    continue
                
                # PhÃ¢n loáº¡i intent
                intent = self.classify_content_by_intent(content, md_file.name)
                print(f"   ğŸ·ï¸ Classified as: {intent}")
                
                # TrÃ­ch xuáº¥t Q&A
                qa_pairs = await self.extract_qa_pairs_from_content(content, intent, md_file.name)
                
                if qa_pairs:
                    # LÆ°u vÃ o file JSON theo intent
                    intent_folder = Path(self.intents[intent]["folder"])
                    output_file = intent_folder / f"{intent}_qa.json"
                    
                    # Merge vá»›i dá»¯ liá»‡u cÅ© náº¿u cÃ³
                    existing_qa_pairs = []
                    if output_file.exists():
                        try:
                            with open(output_file, "r", encoding="utf-8") as f:
                                existing_data = json.load(f)
                            existing_qa_pairs = existing_data.get("qa_pairs", [])
                        except Exception as e:
                            print(f"   âš ï¸ Error reading existing file: {e}")
                    
                    # Táº¡o dá»¯ liá»‡u má»›i
                    all_qa_pairs = existing_qa_pairs + qa_pairs
                    intent_data = {
                        "intent": intent,
                        "description": self.intents[intent]["description"],
                        "count": len(all_qa_pairs),
                        "qa_pairs": all_qa_pairs,
                        "source": "crawled_markdown_files",
                        "created_date": "2025-01-27",
                        "last_updated": "2025-01-27"
                    }
                    
                    # LÆ°u file
                    with open(output_file, "w", encoding="utf-8") as f:
                        json.dump(intent_data, f, indent=2, ensure_ascii=False)
                    
                    print(f"   âœ… Extracted {len(qa_pairs)} Q&A pairs â†’ {output_file}")
                    total_qa_pairs += len(qa_pairs)
                    intent_counts[intent] += len(qa_pairs)
                else:
                    print(f"   âŒ No Q&A pairs extracted")
                
            except Exception as e:
                print(f"   âŒ Error processing {md_file.name}: {e}")
        
        # Táº¡o dataset tá»•ng há»£p
        await self.create_combined_dataset(intent_counts, total_qa_pairs)
        
        print(f"\nğŸ‰ Processing completed!")
        print(f"ğŸ“Š Total Q&A pairs extracted: {total_qa_pairs}")
        print(f"ğŸ“‹ Distribution by intent:")
        for intent, count in intent_counts.items():
            if count > 0:
                print(f"   - {intent}: {count} pairs")
    
    async def create_combined_dataset(self, intent_counts: Dict[str, int], total_qa_pairs: int):
        """Táº¡o dataset tá»•ng há»£p tá»« táº¥t cáº£ cÃ¡c intent"""
        combined_data = {
            "dataset_info": {
                "name": "CTU Comprehensive QA Dataset - Extended",
                "version": "3.0",
                "description": "Dataset cÃ¢u há»i-tráº£ lá»i tÆ° váº¥n tuyá»ƒn sinh Äáº¡i há»c Cáº§n ThÆ¡ tá»« crawled data vá»›i 12 intent categories",
                "created_date": "2025-01-27",
                "source": "Extracted from crawled markdown files",
                "total_pairs": total_qa_pairs,
                "total_intents": len(self.intents),
                "intent_list": list(self.intents.keys())
            },
            "intents": intent_counts,
            "qa_pairs": []
        }
        
        # Äá»c dá»¯ liá»‡u tá»« táº¥t cáº£ cÃ¡c intent
        for intent, intent_info in self.intents.items():
            intent_file = Path(intent_info["folder"]) / f"{intent}_qa.json"
            
            if intent_file.exists():
                try:
                    with open(intent_file, "r", encoding="utf-8") as f:
                        intent_data = json.load(f)
                    
                    qa_pairs = intent_data.get("qa_pairs", [])
                    combined_data["qa_pairs"].extend(qa_pairs)
                    
                    print(f"   âœ… Added {len(qa_pairs)} Q&A pairs from {intent}")
                except Exception as e:
                    print(f"   âŒ Error reading {intent_file}: {e}")
        
        # LÆ°u dataset tá»•ng há»£p
        output_file = Path("data/final/ctu_extended_dataset.json")
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(combined_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Combined dataset saved to: {output_file}")


async def main():
    """Main function"""
    load_dotenv(override=True)
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ No OpenAI API key found. Set OPENAI_API_KEY environment variable.")
        return
    
    print("ğŸš€ Starting extraction from crawled markdown files...")
    
    # Táº¡o extractor
    extractor = CrawledFilesExtractor(api_key)
    
    # Xá»­ lÃ½ táº¥t cáº£ file Ä‘Ã£ cÃ o
    await extractor.process_crawled_files()
    
    print("\nğŸ‰ Extraction from crawled files completed!")


if __name__ == "__main__":
    asyncio.run(main()) 
import asyncio
import json
import os
import re
from pathlib import Path
from typing import Dict, List, Any
from dotenv import load_dotenv
import openai


class IntentBasedExtractor:
    def __init__(self, api_key: str):
        self.client = openai.AsyncOpenAI(api_key=api_key)
        self.intents = {
            "nganh_hoc": {
                "folder": "data/processed/nganh_hoc",
                "description": "Th√¥ng tin v·ªÅ c√°c ng√†nh h·ªçc, ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o",
                "keywords": ["ng√†nh", "chuy√™n ng√†nh", "ƒë√†o t·∫°o", "ch∆∞∆°ng tr√¨nh", "khoa", "b·∫±ng c·∫•p"]
            },
            "xet_tuyen": {
                "folder": "data/processed/xet_tuyen", 
                "description": "Ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn, ƒëi·ªÅu ki·ªán, th·ªß t·ª•c",
                "keywords": ["x√©t tuy·ªÉn", "ph∆∞∆°ng th·ª©c", "ƒëi·ªÅu ki·ªán", "th·ªß t·ª•c", "h·ªì s∆°", "ƒëƒÉng k√Ω"]
            },
            "hoc_phi": {
                "folder": "data/processed/hoc_phi",
                "description": "H·ªçc ph√≠, chi ph√≠, h·ªçc b·ªïng",
                "keywords": ["h·ªçc ph√≠", "chi ph√≠", "h·ªçc b·ªïng", "mi·ªÖn gi·∫£m", "t√†i ch√≠nh"]
            },
            "lien_he": {
                "folder": "data/processed/lien_he",
                "description": "Th√¥ng tin li√™n h·ªá, ƒë·ªãa ch·ªâ, ƒëi·ªán tho·∫°i",
                "keywords": ["li√™n h·ªá", "ƒë·ªãa ch·ªâ", "ƒëi·ªán tho·∫°i", "email", "t∆∞ v·∫•n"]
            },
            "thong_tin": {
                "folder": "data/processed/thong_tin",
                "description": "Th√¥ng tin chung v·ªÅ tr∆∞·ªùng, c∆° s·ªü v·∫≠t ch·∫•t",
                "keywords": ["gi·ªõi thi·ªáu", "l·ªãch s·ª≠", "c∆° s·ªü", "th√¥ng tin chung", "t·∫ßm nh√¨n", "s·ª© m·ªánh"]
            }
        }
    
    def create_intent_folders(self):
        """T·∫°o c√°c th∆∞ m·ª•c theo intent"""
        for intent_name, intent_info in self.intents.items():
            folder_path = Path(intent_info["folder"])
            folder_path.mkdir(parents=True, exist_ok=True)
            print(f"Created folder: {folder_path}")
    
    def classify_content_by_intent(self, content: str) -> Dict[str, str]:
        """Ph√¢n lo·∫°i n·ªôi dung theo intent d·ª±a tr√™n keywords"""
        intent_contents = {intent: "" for intent in self.intents.keys()}
        
        # Chia content th√†nh c√°c ƒëo·∫°n
        paragraphs = content.split('\n\n')
        
        for paragraph in paragraphs:
            if len(paragraph.strip()) < 50:  # B·ªè qua ƒëo·∫°n qu√° ng·∫Øn
                continue
                
            # T√¨m intent ph√π h·ª£p nh·∫•t
            best_intent = "thong_tin"  # Default intent
            max_score = 0
            
            for intent_name, intent_info in self.intents.items():
                score = 0
                for keyword in intent_info["keywords"]:
                    score += paragraph.lower().count(keyword.lower())
                
                if score > max_score:
                    max_score = score
                    best_intent = intent_name
            
            intent_contents[best_intent] += paragraph + "\n\n"
        
        return intent_contents
    
    async def extract_qa_pairs_for_intent(self, content: str, intent: str) -> List[Dict[str, Any]]:
        """Tr√≠ch xu·∫•t Q&A pairs cho m·ªôt intent c·ª• th·ªÉ"""
        if not content.strip():
            return []
        
        intent_info = self.intents[intent]
        
        prompt = f"""
        B·∫°n l√† chuy√™n gia tr√≠ch xu·∫•t d·ªØ li·ªáu cho chatbot t∆∞ v·∫•n tuy·ªÉn sinh ƒê·∫°i h·ªçc C·∫ßn Th∆°.
        
        T·ª´ n·ªôi dung sau v·ªÅ "{intent_info['description']}", h√£y t·∫°o c√°c c·∫∑p c√¢u h·ªèi-tr·∫£ l·ªùi (Q&A) b·∫±ng ti·∫øng Vi·ªát:
        
        N·ªòI DUNG:
        {content[:4000]}  # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° token limit
        
        Y√äU C·∫¶U:
        1. T·∫°o 5-10 c·∫∑p c√¢u h·ªèi-tr·∫£ l·ªùi t·ª± nhi√™n
        2. C√¢u h·ªèi ph·∫£i nh∆∞ sinh vi√™n th·∫≠t s·∫Ω h·ªèi
        3. C√¢u tr·∫£ l·ªùi ph·∫£i ch√≠nh x√°c, d·ª±a tr√™n n·ªôi dung ƒë√£ cho
        4. T·∫≠p trung v√†o ch·ªß ƒë·ªÅ: {intent_info['description']}
        
        Tr·∫£ v·ªÅ JSON format:
        {{
            "intent": "{intent}",
            "qa_pairs": [
                {{
                    "question": "C√¢u h·ªèi v√≠ d·ª•?",
                    "answer": "C√¢u tr·∫£ l·ªùi chi ti·∫øt",
                    "category": "{intent}",
                    "confidence": 0.9
                }}
            ]
        }}
        """
        
        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "B·∫°n l√† chuy√™n gia tr√≠ch xu·∫•t d·ªØ li·ªáu cho chatbot t∆∞ v·∫•n tuy·ªÉn sinh. Lu√¥n tr·∫£ v·ªÅ JSON h·ª£p l·ªá."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # L√†m s·∫°ch JSON response
            if result_text.startswith("```json"):
                result_text = result_text[7:]
            if result_text.endswith("```"):
                result_text = result_text[:-3]
            
            result = json.loads(result_text)
            return result.get("qa_pairs", [])
            
        except Exception as e:
            print(f"Error extracting Q&A for intent {intent}: {e}")
            return []
    
    async def process_markdown_file(self, markdown_file: str):
        """X·ª≠ l√Ω file markdown v√† t·∫°o d·ªØ li·ªáu theo intent"""
        print(f"Processing: {markdown_file}")
        
        # ƒê·ªçc file markdown
        try:
            with open(markdown_file, "r", encoding="utf-8") as f:
                content = f.read()
        except Exception as e:
            print(f"Error reading file: {e}")
            return
        
        print(f"Content length: {len(content)} characters")
        
        # T·∫°o th∆∞ m·ª•c
        self.create_intent_folders()
        
        # Ph√¢n lo·∫°i n·ªôi dung theo intent
        intent_contents = self.classify_content_by_intent(content)
        
        # X·ª≠ l√Ω t·ª´ng intent
        for intent, intent_content in intent_contents.items():
            if not intent_content.strip():
                print(f"No content found for intent: {intent}")
                continue
            
            print(f"\nProcessing intent: {intent}")
            print(f"Content length: {len(intent_content)} characters")
            
            # Tr√≠ch xu·∫•t Q&A pairs
            qa_pairs = await self.extract_qa_pairs_for_intent(intent_content, intent)
            
            if qa_pairs:
                # T·∫°o d·ªØ li·ªáu JSON cho intent
                intent_data = {
                    "intent": intent,
                    "description": self.intents[intent]["description"],
                    "count": len(qa_pairs),
                    "qa_pairs": qa_pairs,
                    "source": "crawl_result.md",
                    "created_date": "2025-01-27"
                }
                
                # L∆∞u v√†o file JSON
                output_file = Path(self.intents[intent]["folder"]) / f"{intent}_qa.json"
                
                # N·∫øu file ƒë√£ t·ªìn t·∫°i, merge d·ªØ li·ªáu
                if output_file.exists():
                    try:
                        with open(output_file, "r", encoding="utf-8") as f:
                            existing_data = json.load(f)
                        
                        # Merge Q&A pairs
                        existing_qa_pairs = existing_data.get("qa_pairs", [])
                        all_qa_pairs = existing_qa_pairs + qa_pairs
                        
                        intent_data["qa_pairs"] = all_qa_pairs
                        intent_data["count"] = len(all_qa_pairs)
                        
                        print(f"Merged with existing data. Total Q&A pairs: {len(all_qa_pairs)}")
                    except Exception as e:
                        print(f"Error reading existing file: {e}")
                
                # L∆∞u file
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(intent_data, f, indent=2, ensure_ascii=False)
                
                print(f"Saved {len(qa_pairs)} Q&A pairs to: {output_file}")
            else:
                print(f"No Q&A pairs extracted for intent: {intent}")
    
    async def create_combined_dataset(self):
        """T·∫°o dataset t·ªïng h·ª£p t·ª´ t·∫•t c·∫£ c√°c intent"""
        combined_data = {
            "dataset_info": {
                "name": "CTU Admission QA Dataset",
                "version": "1.0",
                "description": "Dataset c√¢u h·ªèi-tr·∫£ l·ªùi t∆∞ v·∫•n tuy·ªÉn sinh ƒê·∫°i h·ªçc C·∫ßn Th∆°",
                "created_date": "2025-01-27",
                "source": "Extracted from crawl_result.md"
            },
            "intents": {},
            "qa_pairs": []
        }
        
        total_qa_pairs = 0
        
        # ƒê·ªçc d·ªØ li·ªáu t·ª´ t·∫•t c·∫£ c√°c intent
        for intent, intent_info in self.intents.items():
            intent_file = Path(intent_info["folder"]) / f"{intent}_qa.json"
            
            if intent_file.exists():
                try:
                    with open(intent_file, "r", encoding="utf-8") as f:
                        intent_data = json.load(f)
                    
                    qa_pairs = intent_data.get("qa_pairs", [])
                    combined_data["intents"][intent] = len(qa_pairs)
                    combined_data["qa_pairs"].extend(qa_pairs)
                    total_qa_pairs += len(qa_pairs)
                    
                    print(f"Added {len(qa_pairs)} Q&A pairs from {intent}")
                except Exception as e:
                    print(f"Error reading {intent_file}: {e}")
        
        combined_data["dataset_info"]["total_pairs"] = total_qa_pairs
        
        # L∆∞u dataset t·ªïng h·ª£p
        output_file = Path("data/final/ctu_combined_dataset.json")
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(combined_data, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ Combined dataset saved to: {output_file}")
        print(f"Total Q&A pairs: {total_qa_pairs}")
        print(f"Intents: {list(combined_data['intents'].keys())}")


async def main():
    """Main function"""
    load_dotenv(override=True)
    
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("Error: No OpenAI API key found. Set OPENAI_API_KEY environment variable.")
        return
    
    markdown_file = "output/crawl_result.md"
    
    if not os.path.exists(markdown_file):
        print(f"Error: {markdown_file} not found.")
        return
    
    # T·∫°o extractor
    extractor = IntentBasedExtractor(api_key)
    
    print("üöÄ Starting intent-based extraction...")
    
    # X·ª≠ l√Ω file markdown
    await extractor.process_markdown_file(markdown_file)
    
    # T·∫°o dataset t·ªïng h·ª£p
    await extractor.create_combined_dataset()
    
    print("\nüéâ Extraction completed!")


if __name__ == "__main__":
    asyncio.run(main()) 
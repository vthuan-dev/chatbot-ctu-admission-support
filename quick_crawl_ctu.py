import asyncio
import os
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from datetime import datetime

async def crawl_ctu_admission_sites():
    """
    Crawl c√°c trang tuy·ªÉn sinh CTU c√≥ th√¥ng tin s·∫µn c√≥ (kh√¥ng c·∫ßn dynamic loading)
    """
    print("üöÄ CRAWL CTU ADMISSION SITES")
    
    # Danh s√°ch URLs tuy·ªÉn sinh CTU v·ªõi th√¥ng tin tƒ©nh
    ctu_urls = [
        {
            'url': 'https://tuyensinh.ctu.edu.vn/',
            'name': 'Trang ch·ªß tuy·ªÉn sinh',
            'priority': 1
        },
        {
            'url': 'https://tuyensinh.ctu.edu.vn/nganh-va-chi-tieu',
            'name': 'Ng√†nh v√† ch·ªâ ti√™u',
            'priority': 1
        },
        {
            'url': 'https://tuyensinh.ctu.edu.vn/chuong-trinh-dao-tao',
            'name': 'Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o',
            'priority': 1
        },
        {
            'url': 'https://tuyensinh.ctu.edu.vn/phuong-thuc-xet-tuyen',
            'name': 'Ph∆∞∆°ng th·ª©c x√©t tuy·ªÉn',
            'priority': 1
        },
        {
            'url': 'https://tuyensinh.ctu.edu.vn/hoc-phi',
            'name': 'H·ªçc ph√≠',
            'priority': 1
        },
        {
            'url': 'https://www.ctu.edu.vn/dao-tao',
            'name': 'ƒê√†o t·∫°o CTU',
            'priority': 2
        },
        {
            'url': 'https://ctc.ctu.edu.vn/',
            'name': 'Ch∆∞∆°ng tr√¨nh ch·∫•t l∆∞·ª£ng cao',
            'priority': 2
        }
    ]
    
    browser_config = BrowserConfig(
        headless=True,
        verbose=True
    )
    
    run_config = CrawlerRunConfig(
        delay_before_return_html=2.0,
        simulate_user=True,
        remove_overlay_elements=True
    )
    
    os.makedirs("output", exist_ok=True)
    all_content = []
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        for i, url_info in enumerate(ctu_urls, 1):
            try:
                url = url_info['url']
                name = url_info['name']
                priority = url_info['priority']
                
                print(f"üì° [{i}/{len(ctu_urls)}] {name}")
                print(f"    üåê {url}")
                
                result = await crawler.arun(url=url, config=run_config)
                
                if result.success and result.markdown:
                    content_length = len(result.markdown)
                    print(f"    ‚úÖ Success: {content_length:,} chars")
                    
                    # Check content quality
                    content_lower = result.markdown.lower()
                    quality_indicators = [
                        'ng√†nh', 'tuy·ªÉn sinh', 'ƒë√†o t·∫°o', 'h·ªçc ph√≠', 
                        'ch·ªâ ti√™u', 'ph∆∞∆°ng th·ª©c', 'x√©t tuy·ªÉn'
                    ]
                    
                    quality_score = sum(1 for indicator in quality_indicators if indicator in content_lower)
                    print(f"    üìä Quality score: {quality_score}/{len(quality_indicators)}")
                    
                    content_info = {
                        'url': url,
                        'name': name,
                        'priority': priority,
                        'content': result.markdown,
                        'length': content_length,
                        'quality_score': quality_score,
                        'timestamp': datetime.now().isoformat()
                    }
                    all_content.append(content_info)
                    
                    # Save individual file
                    safe_name = name.replace(' ', '_').replace('/', '_')
                    individual_file = f"output/ctu_{priority}_{safe_name}.md"
                    with open(individual_file, 'w', encoding='utf-8') as f:
                        f.write(f"# {name}\n")
                        f.write(f"URL: {url}\n")
                        f.write(f"Crawled: {datetime.now()}\n")
                        f.write(f"Quality Score: {quality_score}/{len(quality_indicators)}\n\n")
                        f.write(result.markdown)
                    
                else:
                    print(f"    ‚ùå Failed: {url}")
                    
            except Exception as e:
                print(f"    ‚ùå Error: {e}")
    
    # Combine all content based on priority
    if all_content:
        # Sort by priority (1 = highest) then by quality score
        all_content.sort(key=lambda x: (x['priority'], -x['quality_score']))
        
        combined_markdown = f"# CTU ADMISSION CRAWL RESULTS\n"
        combined_markdown += f"Generated: {datetime.now()}\n"
        combined_markdown += f"Total sources: {len(all_content)}\n\n"
        
        # Add summary
        combined_markdown += "## SUMMARY\n\n"
        for content_info in all_content:
            combined_markdown += f"- **{content_info['name']}** (Priority {content_info['priority']})\n"
            combined_markdown += f"  - Quality: {content_info['quality_score']}/7\n"
            combined_markdown += f"  - Length: {content_info['length']:,} chars\n"
            combined_markdown += f"  - URL: {content_info['url']}\n\n"
        
        combined_markdown += "\n---\n\n"
        
        # Add content with priority 1 first
        high_priority_content = [c for c in all_content if c['priority'] == 1]
        medium_priority_content = [c for c in all_content if c['priority'] == 2]
        
        for content_group, group_name in [(high_priority_content, "HIGH PRIORITY"), (medium_priority_content, "MEDIUM PRIORITY")]:
            if content_group:
                combined_markdown += f"# {group_name} CONTENT\n\n"
                
                for content_info in content_group:
                    combined_markdown += f"## {content_info['name']}\n"
                    combined_markdown += f"**URL:** {content_info['url']}\n"
                    combined_markdown += f"**Quality Score:** {content_info['quality_score']}/7\n"
                    combined_markdown += f"**Crawled:** {content_info['timestamp']}\n\n"
                    combined_markdown += content_info['content']
                    combined_markdown += "\n\n---\n\n"
        
        # Save combined file
        combined_file = "output/ctu_admission_combined.md"
        with open(combined_file, "w", encoding="utf-8") as f:
            f.write(combined_markdown)
        
        print(f"\nüéä CRAWL COMPLETED!")
        print(f"üìÑ Combined file: {combined_file}")
        print(f"üìä Total content: {len(combined_markdown):,} characters")
        print(f"üéØ High priority sources: {len(high_priority_content)}")
        print(f"üìù Medium priority sources: {len(medium_priority_content)}")
        
        # Display top quality sources
        print(f"\nüåü TOP QUALITY SOURCES:")
        top_sources = sorted(all_content, key=lambda x: -x['quality_score'])[:3]
        for i, source in enumerate(top_sources, 1):
            print(f"  {i}. {source['name']} (Score: {source['quality_score']}/7)")
            print(f"     üìè {source['length']:,} chars - {source['url']}")
        
        print(f"\nüîß NEXT STEP:")
        print(f"   python 2.llm_extract.py")
        print(f"   (S·∫Ω s·ª≠ d·ª•ng file: {combined_file})")
        
        return combined_file
    
    return None

if __name__ == "__main__":
    asyncio.run(crawl_ctu_admission_sites()) 